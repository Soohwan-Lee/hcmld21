{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"HASYv2/hasy-data/v2-00010.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"HASYv2/hasy-data/v2-00010.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from PIL import Image as pil_image\n",
    "import keras.preprocessing.image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "classes = []\n",
    "with open(\"HASYv2/hasy-data-labels.csv\") as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    i = 0\n",
    "    for row in csvreader:\n",
    "        if i > 0:\n",
    "            img = keras.preprocessing.image.img_to_array(pil_image.open(\"HASYv2/\" + row[0]))\n",
    "            img /= 255.0\n",
    "            imgs.append((row[0],row[2],img))\n",
    "            classes.append(row[2])\n",
    "        i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hasy-data/v2-00000.png',\n",
       " 'A',\n",
       " array([[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       " \n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       " \n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       " \n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       " \n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]], dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168233"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(imgs)\n",
    "split_idx = int(0.8*len(imgs))\n",
    "train = imgs[:split_idx]\n",
    "test = imgs[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_input = np.asarray(list(map(lambda row: row[2], train)))\n",
    "test_input = np.asarray(list(map(lambda row: row[2], test)))\n",
    "\n",
    "train_output = np.asarray(list(map(lambda row: row[1], train)))\n",
    "test_output = np.asarray(list(map(lambda row: row[1], test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 369\n"
     ]
    }
   ],
   "source": [
    "# convert class names into one-hot encoding\n",
    "\n",
    "# first, convert class names into integers\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(classes)\n",
    "\n",
    "# then convert integers into one-hot encoding\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoder.fit(integer_encoded)\n",
    "\n",
    "# convert train and test output to one-hot\n",
    "train_output_int = label_encoder.transform(train_output)\n",
    "train_output = onehot_encoder.transform(train_output_int.reshape(len(train_output_int), 1))\n",
    "test_output_int = label_encoder.transform(test_output)\n",
    "test_output = onehot_encoder.transform(test_output_int.reshape(len(test_output_int), 1))\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Number of classes: %d\" % num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D,MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              1180672   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 369)               378225    \n",
      "=================================================================\n",
      "Total params: 1,569,041\n",
      "Trainable params: 1,569,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3,3), activation = 'relu', input_shape=np.shape(train_input[0])))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(32, (3,3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation = 'tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.callbacks\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir = '.\\logs\\mnist-style')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3365/3365 - 39s - loss: 1.5302 - accuracy: 0.6296 - val_loss: 1.0023 - val_accuracy: 0.7288\n",
      "Epoch 2/10\n",
      "3365/3365 - 38s - loss: 0.9734 - accuracy: 0.7310 - val_loss: 0.9057 - val_accuracy: 0.7520\n",
      "Epoch 3/10\n",
      "3365/3365 - 39s - loss: 0.8576 - accuracy: 0.7549 - val_loss: 0.8713 - val_accuracy: 0.7558\n",
      "Epoch 4/10\n",
      "3365/3365 - 39s - loss: 0.7904 - accuracy: 0.7688 - val_loss: 0.8690 - val_accuracy: 0.7605\n",
      "Epoch 5/10\n",
      "3365/3365 - 38s - loss: 0.7385 - accuracy: 0.7794 - val_loss: 0.8391 - val_accuracy: 0.7680\n",
      "Epoch 6/10\n",
      "3365/3365 - 39s - loss: 0.6964 - accuracy: 0.7880 - val_loss: 0.8269 - val_accuracy: 0.7683\n",
      "Epoch 7/10\n",
      "3365/3365 - 39s - loss: 0.6561 - accuracy: 0.7968 - val_loss: 0.8527 - val_accuracy: 0.7687\n",
      "Epoch 8/10\n",
      "3365/3365 - 39s - loss: 0.6317 - accuracy: 0.8025 - val_loss: 0.8552 - val_accuracy: 0.7674\n",
      "Epoch 9/10\n",
      "3365/3365 - 39s - loss: 0.6052 - accuracy: 0.8093 - val_loss: 0.8758 - val_accuracy: 0.7570\n",
      "Epoch 10/10\n",
      "3365/3365 - 40s - loss: 0.5842 - accuracy: 0.8132 - val_loss: 0.8861 - val_accuracy: 0.7615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2d162d4ac88>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_input, train_output,\n",
    "         batch_size = 32,\n",
    "         epochs = 10,\n",
    "         verbose = 2,\n",
    "         validation_split = 0.2,\n",
    "         callbacks = [tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try various model configurations and parameters to find the best\n",
    "# warning: it take some time\n",
    "\n",
    "import time\n",
    "\n",
    "results = []\n",
    "for conv2d_count in [1,2]:\n",
    "    for dense_size in [128,256,512,1024,2048]:\n",
    "        for dropout in [0.0, 0.25, 0.50, 0.75]:\n",
    "            model = Sequential()\n",
    "            for i in range(conv2d_count):\n",
    "                if i == 0:\n",
    "                    model.add(Conv2D(32, kernel_size = (3,3), activation = 'relu', input_shape = np.shape(train_input[0])))\n",
    "                else:\n",
    "                    model.add(Conv2D(32, kernel_size=(3,3), activation = 'relu'))\n",
    "                model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "            model.add(Flatten())\n",
    "            model.add(Dense(dense_size, activation = 'tanh'))\n",
    "            if dropout > 0.0:\n",
    "                model.add(Dropout(dropout))\n",
    "            model.add(Dense(num_classes, activation = 'softmax'))\n",
    "            \n",
    "            model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "            \n",
    "            log_dir = '.\\logs\\conv2d_%d-dense_%d-dropout_%.2f' % (conv2d_count, dense_size, dropout)\n",
    "            tensorboard = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "                              \n",
    "            start = time.time()\n",
    "            model.fit(train_input, train_output, batch_size=32, epochs=10, verbose=0, validation_split=0.2, callbacks = [tensorboard])\n",
    "            score = model.evaluate(test_input, test_output, verbose=2)\n",
    "            end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 13, 13, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 128)               147584    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 369)               47601     \n",
      "=================================================================\n",
      "Total params: 205,329\n",
      "Trainable params: 205,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      " - 44s - loss: 1.7785 - accuracy: 0.5875\n",
      "Epoch 2/10\n",
      " - 46s - loss: 1.0739 - accuracy: 0.7075\n",
      "Epoch 3/10\n",
      " - 46s - loss: 0.9620 - accuracy: 0.7314\n",
      "Epoch 4/10\n",
      " - 47s - loss: 0.9001 - accuracy: 0.7444\n",
      "Epoch 5/10\n",
      " - 47s - loss: 0.8624 - accuracy: 0.7514\n",
      "Epoch 6/10\n",
      " - 47s - loss: 0.8328 - accuracy: 0.7573\n",
      "Epoch 7/10\n",
      " - 47s - loss: 0.8125 - accuracy: 0.7639\n",
      "Epoch 8/10\n",
      " - 47s - loss: 0.7937 - accuracy: 0.7665\n",
      "Epoch 9/10\n",
      " - 47s - loss: 0.7802 - accuracy: 0.7696\n",
      "Epoch 10/10\n",
      " - 47s - loss: 0.7697 - accuracy: 0.7716\n"
     ]
    }
   ],
   "source": [
    "# rebuild / retrain a model with the best parameters (from the search) and user all data\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size = (3,3), activation = 'relu', input_shape=np.shape(train_input[0])))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(32, (3,3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation = 'tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation = 'softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# join train and test data so we train the network on all data we have available to us\n",
    "model.fit(np.concatenate((train_input, test_input)), np.concatenate((train_output, test_output)), batch_size=32, epochs=10, verbose=2)\n",
    "\n",
    "# save the trained model\n",
    "model.save(\"mathsymbols.model\")\n",
    "\n",
    "# save label encoder (to reverse one-hot encoding)\n",
    "np.save('classes.npy', label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 13, 13, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 128)               147584    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 369)               47601     \n",
      "=================================================================\n",
      "Total params: 205,329\n",
      "Trainable params: 205,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model and predict the math symbol for an arbitary image;\n",
    "# the code below could be placed in a separate file\n",
    "\n",
    "import keras.models\n",
    "model2 = keras.models.load_model(\"mathsymbols.model\")\n",
    "print(model2.summary())\n",
    "\n",
    "#restore the class name to integer encoder\n",
    "label_encoder2 = LabelEncoder()\n",
    "label_encoder2.classes_ = np.load('classes.npy')\n",
    "\n",
    "def predict(img_path):\n",
    "    newimg = keras.preprocessing.image.img_to_array(pil_image.open(img_path))\n",
    "    newimg /= 255.0\n",
    "    \n",
    "    # do the prediction\n",
    "    prediction = model2.predict(newimg.reshape(1,32,32,3))\n",
    "    \n",
    "    #figure out which output neuron had the highest score, and reverse the one-hot encoding\n",
    "    inverted = label_encoder2.inverse_transform([np.argmax(prediction)]) # argmax finds highest-scoring output\n",
    "    print(\"Prediction: %s, confidence: %.2f\" % (inverted[0], np.max(prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 29, 29, 32)        1568      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 11, 32)        16416     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               102528    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 369)               47601     \n",
      "=================================================================\n",
      "Total params: 168,113\n",
      "Trainable params: 168,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model and predict the math symbol for an arbitary image;\n",
    "# the code below could be placed in a separate file\n",
    "\n",
    "import keras.models\n",
    "model2 = keras.models.load_model(\"mathsymbols2.model\")\n",
    "print(model2.summary())\n",
    "\n",
    "#restore the class name to integer encoder\n",
    "label_encoder2 = LabelEncoder()\n",
    "label_encoder2.classes_ = np.load('classes2.npy')\n",
    "\n",
    "def predict2(img_path):\n",
    "    newimg = keras.preprocessing.image.img_to_array(pil_image.open(img_path))\n",
    "    newimg /= 255.0\n",
    "    \n",
    "    # do the prediction\n",
    "    prediction = model2.predict(newimg.reshape(1,32,32,4))\n",
    "    \n",
    "    #figure out which output neuron had the highest score, and reverse the one-hot encoding\n",
    "    inverted = label_encoder2.inverse_transform([np.argmax(prediction)]) # argmax finds highest-scoring output\n",
    "    print(\"Prediction: %s, confidence: %.2f\" % (inverted[0], np.max(prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: A, confidence: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Example: grab an image (we'll just use a random training image for demonstration purposes)\n",
    "predict(\"HASYv2/hasy-data/v2-00010.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My sample: A B C D E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"HASYv2/testData/0.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"HASYv2/testData/0.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"HASYv2/testData/1.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"HASYv2/testData/1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"HASYv2/testData/2.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"HASYv2/testData/2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"HASYv2/testData/3.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"HASYv2/testData/3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"HASYv2/testData/4.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"HASYv2/testData/4.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: \\mathcal{A}, confidence: 0.60\n",
      "Prediction: \\beta, confidence: 0.33\n",
      "Prediction: \\subseteq, confidence: 0.74\n",
      "Prediction: \\mathcal{D}, confidence: 0.72\n",
      "Prediction: \\vDash, confidence: 0.29\n"
     ]
    }
   ],
   "source": [
    "predict(\"HASYv2/testData/0.png\")\n",
    "predict(\"HASYv2/testData/1.png\")\n",
    "predict(\"HASYv2/testData/2.png\")\n",
    "predict(\"HASYv2/testData/3.png\")\n",
    "predict(\"HASYv2/testData/4.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revisiting the bird species identifier to use images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Input, Dropout, Flatten, Conv2D, MaxPooling2D, Dense, Activation, GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import TensorBoard\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all images will be converted to this size\n",
    "ROWS = 256\n",
    "COLS = 256\n",
    "CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5990 5790\n",
      "Dataset succesfully splitted!\n"
     ]
    }
   ],
   "source": [
    "# split data set\n",
    "# REF: https://github.com/irfanICMLL/inceptionV2_finetune\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "\n",
    "# Change the path to your dataset folder:\n",
    "base_folder = './CUB_200_2011/'\n",
    "\n",
    "# These path should be fine\n",
    "images_txt_path = base_folder+ 'images.txt'\n",
    "train_test_split_path =  base_folder+ 'train_test_split.txt'\n",
    "images_path =  base_folder+ 'images/'\n",
    "\n",
    "# Here declare where you want to place the train/test folders\n",
    "# You don't need to create them!\n",
    "test_folder = './test/'\n",
    "train_folder = './train/'\n",
    "\n",
    "\n",
    "def ignore_files(dir,files): return [f for f in files if os.path.isfile(os.path.join(dir,f))]\n",
    "\n",
    "shutil.copytree(images_path,test_folder,ignore=ignore_files)\n",
    "shutil.copytree(images_path,train_folder,ignore=ignore_files)\n",
    "\n",
    "with open(images_txt_path) as f:\n",
    "  images_lines = f.readlines()\n",
    "\n",
    "with open(train_test_split_path) as f:\n",
    "  split_lines = f.readlines()\n",
    "\n",
    "test_images, train_images = 0,0\n",
    "\n",
    "for image_line,split_line in zip(images_lines,split_lines):\n",
    "\n",
    "  image_line = (image_line.strip()).split(' ')\n",
    "  split_line = (split_line.strip()).split(' ')\n",
    "\n",
    "  image = plt.imread(images_path + image_line[1])\n",
    "\n",
    "  # Use only RGB images, avoid grayscale\n",
    "  if len(image.shape) == 3:\n",
    "\n",
    "    # If test image\n",
    "    if(int(split_line[1]) is 0):\n",
    "      copyfile(images_path+image_line[1],test_folder+image_line[1])\n",
    "      test_images += 1 \n",
    "    else:\n",
    "      # If train image\n",
    "      copyfile(images_path+image_line[1],train_folder+image_line[1])\n",
    "      train_images += 1 \n",
    "\n",
    "print(train_images,test_images)\n",
    "assert train_images == 5990\n",
    "assert test_images == 5790\n",
    "\n",
    "print('Dataset succesfully splitted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5990 images belonging to 200 classes.\n",
      "Found 5790 images belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "train_image_generator = ImageDataGenerator(horizontal_flip=True, rescale=1./255, rotation_range=45)\n",
    "test_image_generator = ImageDataGenerator(horizontal_flip=False, rescale=1./255, rotation_range=0)\n",
    "\n",
    "train_generator = train_image_generator.flow_from_directory('train', target_size=(ROWS, COLS), class_mode='categorical')\n",
    "test_generator = test_image_generator.flow_from_directory('test', target_size=(ROWS, COLS), class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 254, 254, 64)      1792      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 254, 254, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 252, 252, 64)      36928     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 252, 252, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 63, 63, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 61, 61, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 61, 61, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 59, 59, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 59, 59, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 400)               5018000   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 200)               0         \n",
      "=================================================================\n",
      "Total params: 5,210,776\n",
      "Trainable params: 5,210,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "train_generator.reset()\n",
    "test_generator.reset()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3,3), input_shape=(ROWS, COLS, CHANNELS)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(4,4)))\n",
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(4,4)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(400))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "128/128 [==============================] - 310s 2s/step - loss: 5.1952 - accuracy: 0.0108\n",
      "Epoch 2/10\n",
      "128/128 [==============================] - 311s 2s/step - loss: 5.1277 - accuracy: 0.0118\n",
      "Epoch 3/10\n",
      "128/128 [==============================] - 314s 2s/step - loss: 5.0402 - accuracy: 0.0187\n",
      "Epoch 4/10\n",
      "128/128 [==============================] - 318s 2s/step - loss: 4.9782 - accuracy: 0.0214\n",
      "Epoch 5/10\n",
      "128/128 [==============================] - 316s 2s/step - loss: 4.9265 - accuracy: 0.0265\n",
      "Epoch 6/10\n",
      "128/128 [==============================] - 312s 2s/step - loss: 4.8500 - accuracy: 0.0295\n",
      "Epoch 7/10\n",
      "128/128 [==============================] - 318s 2s/step - loss: 4.7729 - accuracy: 0.0344\n",
      "Epoch 8/10\n",
      "128/128 [==============================] - 312s 2s/step - loss: 4.7011 - accuracy: 0.0391\n",
      "Epoch 9/10\n",
      "128/128 [==============================] - 313s 2s/step - loss: 4.6532 - accuracy: 0.0523\n",
      "Epoch 10/10\n",
      "128/128 [==============================] - 311s 2s/step - loss: 4.5665 - accuracy: 0.0543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ceccf92c88>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard = TensorBoard(log_dir='./logs/custom')\n",
    "\n",
    "model.fit_generator(train_generator, steps_per_epoch=128, epochs=10, callbacks=[tensorboard], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.evaluate_generator(test_generator, steps=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "out_layer = Dense(200, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=out_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, None, None, 3 864         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, None, None, 3 96          conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, None, None, 3 0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, None, None, 3 9216        activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, None, None, 3 96          conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, None, None, 3 0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, None, None, 6 18432       activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, None, None, 6 192         conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, None, None, 6 0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, None, None, 6 0           activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, None, None, 8 5120        max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, None, None, 8 240         conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, None, None, 8 0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, None, None, 1 138240      activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, None, None, 1 576         conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, None, None, 1 0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, None, None, 1 0           activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, None, None, 6 12288       max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, None, None, 6 192         conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, None, None, 6 0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, None, None, 4 9216        max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, None, None, 9 55296       activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, None, None, 4 144         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, None, None, 9 288         conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, None, None, 4 0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, None, None, 9 0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, None, None, 1 0           max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, None, None, 6 12288       max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, None, None, 6 76800       activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, None, None, 9 82944       activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, None, None, 3 6144        average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, None, None, 6 192         conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, None, None, 6 192         conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, None, None, 9 288         conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, None, None, 3 96          conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, None, None, 6 0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, None, None, 6 0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, None, None, 9 0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, None, None, 3 0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, None, None, 2 0           activation_105[0][0]             \n",
      "                                                                 activation_107[0][0]             \n",
      "                                                                 activation_110[0][0]             \n",
      "                                                                 activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, None, None, 6 16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, None, None, 6 192         conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, None, None, 6 0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, None, None, 4 12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, None, None, 9 55296       activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, None, None, 4 144         conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, None, None, 9 288         conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, None, None, 4 0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, None, None, 9 0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_10 (AveragePo (None, None, None, 2 0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, None, None, 6 16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, None, None, 6 76800       activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, None, None, 9 82944       activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, None, None, 6 16384       average_pooling2d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, None, None, 6 192         conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, None, None, 6 192         conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, None, None, 9 288         conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, None, None, 6 192         conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, None, None, 6 0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, None, None, 6 0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, None, None, 9 0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, None, None, 6 0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, None, None, 2 0           activation_112[0][0]             \n",
      "                                                                 activation_114[0][0]             \n",
      "                                                                 activation_117[0][0]             \n",
      "                                                                 activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, None, None, 6 18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, None, None, 6 192         conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, None, None, 6 0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, None, None, 4 13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, None, None, 9 55296       activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, None, None, 4 144         conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, None, None, 9 288         conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, None, None, 4 0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, None, None, 9 0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_11 (AveragePo (None, None, None, 2 0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, None, None, 6 18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, None, None, 6 76800       activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, None, None, 9 82944       activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, None, None, 6 18432       average_pooling2d_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, None, None, 6 192         conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, None, None, 6 192         conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, None, None, 9 288         conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, None, None, 6 192         conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, None, None, 6 0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, None, None, 6 0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, None, None, 9 0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, None, None, 6 0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, None, None, 2 0           activation_119[0][0]             \n",
      "                                                                 activation_121[0][0]             \n",
      "                                                                 activation_124[0][0]             \n",
      "                                                                 activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, None, None, 6 18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, None, None, 6 192         conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, None, None, 6 0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, None, None, 9 55296       activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, None, None, 9 288         conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, None, None, 9 0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, None, None, 3 995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, None, None, 9 82944       activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, None, None, 3 1152        conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, None, None, 9 288         conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, None, None, 3 0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, None, None, 9 0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, None, None, 2 0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, None, None, 7 0           activation_126[0][0]             \n",
      "                                                                 activation_129[0][0]             \n",
      "                                                                 max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, None, None, 1 98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, None, None, 1 384         conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, None, None, 1 0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, None, None, 1 114688      activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, None, None, 1 384         conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, None, None, 1 0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, None, None, 1 98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, None, None, 1 114688      activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, None, None, 1 384         conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, None, None, 1 384         conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, None, None, 1 0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, None, None, 1 0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, None, None, 1 114688      activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, None, None, 1 114688      activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, None, None, 1 384         conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, None, None, 1 384         conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, None, None, 1 0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, None, None, 1 0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_12 (AveragePo (None, None, None, 7 0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, None, None, 1 147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, None, None, 1 172032      activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, None, None, 1 172032      activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, None, None, 1 147456      average_pooling2d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, None, None, 1 576         conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, None, None, 1 576         conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, None, None, 1 576         conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, None, None, 1 576         conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, None, None, 1 0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, None, None, 1 0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, None, None, 1 0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, None, None, 1 0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, None, None, 7 0           activation_130[0][0]             \n",
      "                                                                 activation_133[0][0]             \n",
      "                                                                 activation_138[0][0]             \n",
      "                                                                 activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, None, None, 1 122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, None, None, 1 480         conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, None, None, 1 0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, None, None, 1 179200      activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, None, None, 1 480         conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, None, None, 1 0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, None, None, 1 122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, None, None, 1 179200      activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, None, None, 1 480         conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, None, None, 1 480         conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, None, None, 1 0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, None, None, 1 0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, None, None, 1 179200      activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, None, None, 1 179200      activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, None, None, 1 480         conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, None, None, 1 480         conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, None, None, 1 0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, None, None, 1 0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_13 (AveragePo (None, None, None, 7 0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, None, None, 1 147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, None, None, 1 215040      activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, None, None, 1 215040      activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, None, None, 1 147456      average_pooling2d_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, None, None, 1 576         conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, None, None, 1 576         conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, None, None, 1 576         conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, None, None, 1 576         conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, None, None, 1 0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, None, None, 1 0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, None, None, 1 0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, None, None, 1 0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, None, None, 7 0           activation_140[0][0]             \n",
      "                                                                 activation_143[0][0]             \n",
      "                                                                 activation_148[0][0]             \n",
      "                                                                 activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, None, None, 1 122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, None, None, 1 480         conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, None, None, 1 0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, None, None, 1 179200      activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, None, None, 1 480         conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, None, None, 1 0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, None, None, 1 122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, None, None, 1 179200      activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, None, None, 1 480         conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, None, None, 1 480         conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, None, None, 1 0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, None, None, 1 0           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, None, None, 1 179200      activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, None, None, 1 179200      activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, None, None, 1 480         conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, None, None, 1 480         conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, None, None, 1 0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, None, None, 1 0           batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_14 (AveragePo (None, None, None, 7 0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, None, None, 1 147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, None, None, 1 215040      activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, None, None, 1 215040      activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, None, None, 1 147456      average_pooling2d_14[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, None, None, 1 576         conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, None, None, 1 576         conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, None, None, 1 576         conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, None, None, 1 576         conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, None, None, 1 0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, None, None, 1 0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, None, None, 1 0           batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, None, None, 1 0           batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, None, None, 7 0           activation_150[0][0]             \n",
      "                                                                 activation_153[0][0]             \n",
      "                                                                 activation_158[0][0]             \n",
      "                                                                 activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, None, None, 1 147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, None, None, 1 576         conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, None, None, 1 0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, None, None, 1 258048      activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, None, None, 1 576         conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, None, None, 1 0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, None, None, 1 147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, None, None, 1 258048      activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, None, None, 1 576         conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, None, None, 1 576         conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, None, None, 1 0           batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, None, None, 1 0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, None, None, 1 258048      activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, None, None, 1 258048      activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, None, None, 1 576         conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, None, None, 1 576         conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, None, None, 1 0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, None, None, 1 0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_15 (AveragePo (None, None, None, 7 0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, None, None, 1 147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, None, None, 1 258048      activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, None, None, 1 258048      activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, None, None, 1 147456      average_pooling2d_15[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, None, None, 1 576         conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, None, None, 1 576         conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, None, None, 1 576         conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, None, None, 1 576         conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, None, None, 1 0           batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, None, None, 1 0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, None, None, 1 0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, None, None, 1 0           batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, None, None, 7 0           activation_160[0][0]             \n",
      "                                                                 activation_163[0][0]             \n",
      "                                                                 activation_168[0][0]             \n",
      "                                                                 activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, None, None, 1 147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, None, None, 1 576         conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, None, None, 1 0           batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, None, None, 1 258048      activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, None, None, 1 576         conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, None, None, 1 0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, None, None, 1 147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, None, None, 1 258048      activation_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, None, None, 1 576         conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, None, None, 1 576         conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, None, None, 1 0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, None, None, 1 0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, None, None, 3 552960      activation_170[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, None, None, 1 331776      activation_174[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, None, None, 3 960         conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, None, None, 1 576         conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, None, None, 3 0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, None, None, 1 0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, None, None, 7 0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, None, None, 1 0           activation_171[0][0]             \n",
      "                                                                 activation_175[0][0]             \n",
      "                                                                 max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_186 (Conv2D)             (None, None, None, 4 573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, None, None, 4 1344        conv2d_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, None, None, 4 0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, None, None, 3 491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_187 (Conv2D)             (None, None, None, 3 1548288     activation_180[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, None, None, 3 1152        conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, None, None, 3 1152        conv2d_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, None, None, 3 0           batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, None, None, 3 0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, None, None, 3 442368      activation_177[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, None, None, 3 442368      activation_177[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_188 (Conv2D)             (None, None, None, 3 442368      activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_189 (Conv2D)             (None, None, None, 3 442368      activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_16 (AveragePo (None, None, None, 1 0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, None, None, 3 409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, None, None, 3 1152        conv2d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, None, None, 3 1152        conv2d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, None, None, 3 1152        conv2d_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, None, None, 3 1152        conv2d_189[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_190 (Conv2D)             (None, None, None, 1 245760      average_pooling2d_16[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, None, None, 3 960         conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_178 (Activation)     (None, None, None, 3 0           batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, None, None, 3 0           batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, None, None, 3 0           batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, None, None, 3 0           batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, None, None, 1 576         conv2d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, None, None, 3 0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, None, None, 7 0           activation_178[0][0]             \n",
      "                                                                 activation_179[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, None, 7 0           activation_182[0][0]             \n",
      "                                                                 activation_183[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_184 (Activation)     (None, None, None, 1 0           batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, None, None, 2 0           activation_176[0][0]             \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_195 (Conv2D)             (None, None, None, 4 917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, None, None, 4 1344        conv2d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_189 (Activation)     (None, None, None, 4 0           batch_normalization_183[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_192 (Conv2D)             (None, None, None, 3 786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_196 (Conv2D)             (None, None, None, 3 1548288     activation_189[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, None, None, 3 1152        conv2d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_184 (BatchN (None, None, None, 3 1152        conv2d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_186 (Activation)     (None, None, None, 3 0           batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_190 (Activation)     (None, None, None, 3 0           batch_normalization_184[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_193 (Conv2D)             (None, None, None, 3 442368      activation_186[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_194 (Conv2D)             (None, None, None, 3 442368      activation_186[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_197 (Conv2D)             (None, None, None, 3 442368      activation_190[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_198 (Conv2D)             (None, None, None, 3 442368      activation_190[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_17 (AveragePo (None, None, None, 2 0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_191 (Conv2D)             (None, None, None, 3 655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, None, None, 3 1152        conv2d_193[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, None, None, 3 1152        conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_185 (BatchN (None, None, None, 3 1152        conv2d_197[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_186 (BatchN (None, None, None, 3 1152        conv2d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_199 (Conv2D)             (None, None, None, 1 393216      average_pooling2d_17[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, None, None, 3 960         conv2d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_187 (Activation)     (None, None, None, 3 0           batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_188 (Activation)     (None, None, None, 3 0           batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_191 (Activation)     (None, None, None, 3 0           batch_normalization_185[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_192 (Activation)     (None, None, None, 3 0           batch_normalization_186[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_187 (BatchN (None, None, None, 1 576         conv2d_199[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_185 (Activation)     (None, None, None, 3 0           batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, None, None, 7 0           activation_187[0][0]             \n",
      "                                                                 activation_188[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, None, 7 0           activation_191[0][0]             \n",
      "                                                                 activation_192[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_193 (Activation)     (None, None, None, 1 0           batch_normalization_187[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, None, None, 2 0           activation_185[0][0]             \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_3[0][0]              \n",
      "                                                                 activation_193[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1024)         2098176     global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 200)          205000      dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 24,105,960\n",
      "Trainable params: 2,303,176\n",
      "Non-trainable params: 21,802,784\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - 26s 721ms/step - loss: 6.9253 - accuracy: 0.0180\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 23s 707ms/step - loss: 4.8185 - accuracy: 0.0749\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 22s 688ms/step - loss: 4.1948 - accuracy: 0.0939\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 22s 695ms/step - loss: 3.6844 - accuracy: 0.1573\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 22s 688ms/step - loss: 3.2513 - accuracy: 0.2328\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 22s 692ms/step - loss: 3.1214 - accuracy: 0.2189\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 22s 691ms/step - loss: 3.0374 - accuracy: 0.2658\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 22s 680ms/step - loss: 2.7920 - accuracy: 0.2961\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 22s 684ms/step - loss: 2.7428 - accuracy: 0.2783\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 22s 686ms/step - loss: 2.4579 - accuracy: 0.3329\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 22s 686ms/step - loss: 2.3434 - accuracy: 0.3984\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 22s 695ms/step - loss: 2.4261 - accuracy: 0.3619\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 22s 692ms/step - loss: 2.2601 - accuracy: 0.3712\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 23s 698ms/step - loss: 2.3056 - accuracy: 0.3746\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 22s 693ms/step - loss: 2.1105 - accuracy: 0.4102\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 22s 691ms/step - loss: 2.1844 - accuracy: 0.4356\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 22s 693ms/step - loss: 2.0000 - accuracy: 0.4372\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 22s 684ms/step - loss: 2.1537 - accuracy: 0.4023\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 22s 693ms/step - loss: 1.8769 - accuracy: 0.4828\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 22s 688ms/step - loss: 1.9276 - accuracy: 0.4252\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 22s 679ms/step - loss: 1.9145 - accuracy: 0.4518\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 22s 697ms/step - loss: 1.8882 - accuracy: 0.4945\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 22s 688ms/step - loss: 1.8003 - accuracy: 0.4728\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 23s 697ms/step - loss: 1.6553 - accuracy: 0.5493\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 23s 700ms/step - loss: 1.6826 - accuracy: 0.5290\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 23s 714ms/step - loss: 1.7421 - accuracy: 0.4995\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 23s 723ms/step - loss: 1.6311 - accuracy: 0.5698\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 23s 701ms/step - loss: 1.6586 - accuracy: 0.5227\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 23s 709ms/step - loss: 1.5961 - accuracy: 0.5306\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 23s 697ms/step - loss: 1.5900 - accuracy: 0.5212\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 22s 692ms/step - loss: 1.6226 - accuracy: 0.5694\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 22s 682ms/step - loss: 1.5832 - accuracy: 0.5545\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 22s 678ms/step - loss: 1.6106 - accuracy: 0.5443\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 22s 686ms/step - loss: 1.6615 - accuracy: 0.5410\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 23s 718ms/step - loss: 1.4530 - accuracy: 0.5687\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - 23s 716ms/step - loss: 1.4966 - accuracy: 0.5730\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 22s 689ms/step - loss: 1.4560 - accuracy: 0.5886\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 23s 696ms/step - loss: 1.4633 - accuracy: 0.6195\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 23s 700ms/step - loss: 1.5617 - accuracy: 0.5576\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 22s 691ms/step - loss: 1.4152 - accuracy: 0.6090\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 22s 692ms/step - loss: 1.3882 - accuracy: 0.6081\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 22s 694ms/step - loss: 1.4443 - accuracy: 0.5870\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 22s 690ms/step - loss: 1.5627 - accuracy: 0.5757\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 22s 691ms/step - loss: 1.4171 - accuracy: 0.6016\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 22s 693ms/step - loss: 1.4652 - accuracy: 0.5695\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 22s 690ms/step - loss: 1.4348 - accuracy: 0.5926\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 22s 691ms/step - loss: 1.3849 - accuracy: 0.6068\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 22s 688ms/step - loss: 1.3245 - accuracy: 0.6139\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 22s 686ms/step - loss: 1.1685 - accuracy: 0.6495\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 23s 707ms/step - loss: 1.3339 - accuracy: 0.6207\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 23s 714ms/step - loss: 1.2496 - accuracy: 0.6329\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 24s 733ms/step - loss: 1.2082 - accuracy: 0.6482\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 23s 722ms/step - loss: 1.3704 - accuracy: 0.6153\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 22s 691ms/step - loss: 1.2896 - accuracy: 0.6241\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 22s 693ms/step - loss: 1.1649 - accuracy: 0.6563\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 22s 690ms/step - loss: 1.3334 - accuracy: 0.6501\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 22s 672ms/step - loss: 1.0603 - accuracy: 0.6923\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 22s 694ms/step - loss: 1.1409 - accuracy: 0.6716\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 22s 678ms/step - loss: 1.2469 - accuracy: 0.6377\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 22s 679ms/step - loss: 1.1895 - accuracy: 0.6441\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 22s 695ms/step - loss: 1.2549 - accuracy: 0.6395\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 22s 672ms/step - loss: 1.2054 - accuracy: 0.6600\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 22s 690ms/step - loss: 1.0712 - accuracy: 0.6908\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 22s 690ms/step - loss: 1.0626 - accuracy: 0.6959\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 23s 696ms/step - loss: 1.2030 - accuracy: 0.6752\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 22s 688ms/step - loss: 1.0480 - accuracy: 0.6826\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 22s 673ms/step - loss: 1.0540 - accuracy: 0.7054\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 22s 677ms/step - loss: 1.1663 - accuracy: 0.6771\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 22s 688ms/step - loss: 0.9875 - accuracy: 0.6964\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 22s 689ms/step - loss: 1.0985 - accuracy: 0.6812\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 22s 687ms/step - loss: 0.9880 - accuracy: 0.6984\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 22s 684ms/step - loss: 0.9806 - accuracy: 0.7027\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 22s 689ms/step - loss: 1.1397 - accuracy: 0.6905\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 22s 691ms/step - loss: 0.9622 - accuracy: 0.7155\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 22s 687ms/step - loss: 1.0265 - accuracy: 0.7090\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 22s 687ms/step - loss: 1.1305 - accuracy: 0.6587\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 22s 692ms/step - loss: 1.1074 - accuracy: 0.6850\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 22s 673ms/step - loss: 0.9655 - accuracy: 0.7139\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - 22s 689ms/step - loss: 1.0637 - accuracy: 0.7103\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 22s 691ms/step - loss: 0.8878 - accuracy: 0.7318\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 23s 695ms/step - loss: 1.0168 - accuracy: 0.6891\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - 23s 696ms/step - loss: 0.9367 - accuracy: 0.7182\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - 23s 697ms/step - loss: 0.9752 - accuracy: 0.7053\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 23s 712ms/step - loss: 0.9288 - accuracy: 0.7279\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 23s 695ms/step - loss: 1.0419 - accuracy: 0.6888\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 23s 696ms/step - loss: 0.9521 - accuracy: 0.7297\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 22s 693ms/step - loss: 1.0042 - accuracy: 0.7153\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 22s 691ms/step - loss: 1.0619 - accuracy: 0.7037\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 22s 691ms/step - loss: 1.0143 - accuracy: 0.6911\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 22s 695ms/step - loss: 1.0516 - accuracy: 0.7039\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 22s 679ms/step - loss: 1.0182 - accuracy: 0.7023\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 23s 695ms/step - loss: 0.8897 - accuracy: 0.7272\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 23s 698ms/step - loss: 0.9421 - accuracy: 0.7344\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - 23s 696ms/step - loss: 1.0233 - accuracy: 0.7189\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - 22s 690ms/step - loss: 0.9999 - accuracy: 0.6998\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - 22s 682ms/step - loss: 0.8703 - accuracy: 0.7335\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - 23s 702ms/step - loss: 0.9826 - accuracy: 0.7187\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - 22s 680ms/step - loss: 1.0006 - accuracy: 0.7266\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - 22s 694ms/step - loss: 1.0791 - accuracy: 0.7082\n",
      "Epoch 100/100\n",
      "32/32 [==============================] - 22s 691ms/step - loss: 0.8828 - accuracy: 0.7544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ced483dc88>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard = TensorBoard(log_dir='./logs/inceptionv3')\n",
    "\n",
    "model.fit_generator(train_generator, steps_per_epoch=32, epochs=100, callbacks=[tensorboard], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.evaluate_generator(test_generator, steps=5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - 138s 4s/step - loss: 2.6111 - accuracy: 0.4017\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 134s 4s/step - loss: 2.0951 - accuracy: 0.4869\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 134s 4s/step - loss: 1.5582 - accuracy: 0.5873\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 134s 4s/step - loss: 1.3347 - accuracy: 0.6049\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 134s 4s/step - loss: 1.2109 - accuracy: 0.6470\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 135s 4s/step - loss: 1.3775 - accuracy: 0.6117\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 136s 4s/step - loss: 1.0367 - accuracy: 0.6911\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 137s 4s/step - loss: 1.0245 - accuracy: 0.6948\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 131s 4s/step - loss: 0.9376 - accuracy: 0.7233\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 135s 4s/step - loss: 0.9905 - accuracy: 0.7115\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 136s 4s/step - loss: 1.0502 - accuracy: 0.6800\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 137s 4s/step - loss: 0.8688 - accuracy: 0.7506\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 135s 4s/step - loss: 0.9793 - accuracy: 0.7299\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 134s 4s/step - loss: 0.9514 - accuracy: 0.7391\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 133s 4s/step - loss: 0.8409 - accuracy: 0.7481\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 136s 4s/step - loss: 0.8082 - accuracy: 0.7735\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 136s 4s/step - loss: 0.7995 - accuracy: 0.7805\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 134s 4s/step - loss: 0.7305 - accuracy: 0.7929\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 136s 4s/step - loss: 0.6985 - accuracy: 0.7906\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 137s 4s/step - loss: 0.7871 - accuracy: 0.7882\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 137s 4s/step - loss: 0.6940 - accuracy: 0.7896\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 133s 4s/step - loss: 0.7078 - accuracy: 0.7891\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 134s 4s/step - loss: 0.7142 - accuracy: 0.8006\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 134s 4s/step - loss: 0.6736 - accuracy: 0.7932\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 133s 4s/step - loss: 0.5751 - accuracy: 0.8391\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 134s 4s/step - loss: 0.6027 - accuracy: 0.8163\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 132s 4s/step - loss: 0.6699 - accuracy: 0.7840\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 134s 4s/step - loss: 0.6089 - accuracy: 0.8233\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 134s 4s/step - loss: 0.5723 - accuracy: 0.8308\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 134s 4s/step - loss: 0.5407 - accuracy: 0.8582\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 134s 4s/step - loss: 0.6032 - accuracy: 0.8421\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 134s 4s/step - loss: 0.5153 - accuracy: 0.8399\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 132s 4s/step - loss: 0.5246 - accuracy: 0.8380\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 135s 4s/step - loss: 0.6333 - accuracy: 0.8211\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 134s 4s/step - loss: 0.5704 - accuracy: 0.8370\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - 134s 4s/step - loss: 0.5982 - accuracy: 0.8073\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 135s 4s/step - loss: 0.5299 - accuracy: 0.8507\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 137s 4s/step - loss: 0.5234 - accuracy: 0.8444\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 137s 4s/step - loss: 0.5039 - accuracy: 0.8598\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 136s 4s/step - loss: 0.4371 - accuracy: 0.8733\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 132s 4s/step - loss: 0.4617 - accuracy: 0.8696\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 132s 4s/step - loss: 0.4511 - accuracy: 0.8569\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 132s 4s/step - loss: 0.4691 - accuracy: 0.8682\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 130s 4s/step - loss: 0.4614 - accuracy: 0.8832\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 128s 4s/step - loss: 0.5574 - accuracy: 0.8378\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 136s 4s/step - loss: 0.4340 - accuracy: 0.8596\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 132s 4s/step - loss: 0.5217 - accuracy: 0.8528\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 126s 4s/step - loss: 0.4309 - accuracy: 0.8796\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 129s 4s/step - loss: 0.4281 - accuracy: 0.8687\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 129s 4s/step - loss: 0.4208 - accuracy: 0.8788\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 129s 4s/step - loss: 0.4243 - accuracy: 0.8720\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 126s 4s/step - loss: 0.4502 - accuracy: 0.8680\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 126s 4s/step - loss: 0.3974 - accuracy: 0.9012\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 130s 4s/step - loss: 0.4659 - accuracy: 0.8556\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 130s 4s/step - loss: 0.3821 - accuracy: 0.8874\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 129s 4s/step - loss: 0.3610 - accuracy: 0.8949\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 129s 4s/step - loss: 0.3745 - accuracy: 0.9021\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 130s 4s/step - loss: 0.3832 - accuracy: 0.8868\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 130s 4s/step - loss: 0.3575 - accuracy: 0.9058\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 130s 4s/step - loss: 0.3486 - accuracy: 0.8942\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 129s 4s/step - loss: 0.4217 - accuracy: 0.8757\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 130s 4s/step - loss: 0.4108 - accuracy: 0.8886\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 130s 4s/step - loss: 0.3274 - accuracy: 0.9082\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 130s 4s/step - loss: 0.3945 - accuracy: 0.8727\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 128s 4s/step - loss: 0.3007 - accuracy: 0.9147\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 127s 4s/step - loss: 0.3504 - accuracy: 0.9048\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 131s 4s/step - loss: 0.4279 - accuracy: 0.8788\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 130s 4s/step - loss: 0.3821 - accuracy: 0.8905\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 131s 4s/step - loss: 0.3361 - accuracy: 0.8936\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 131s 4s/step - loss: 0.3241 - accuracy: 0.9003\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 131s 4s/step - loss: 0.3537 - accuracy: 0.8889\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 130s 4s/step - loss: 0.3709 - accuracy: 0.8963\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 131s 4s/step - loss: 0.2789 - accuracy: 0.9179\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 141s 4s/step - loss: 0.2946 - accuracy: 0.9064\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 144s 4s/step - loss: 0.2947 - accuracy: 0.9214\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 142s 4s/step - loss: 0.3485 - accuracy: 0.9137\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 144s 5s/step - loss: 0.3534 - accuracy: 0.8965\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 136s 4s/step - loss: 0.3169 - accuracy: 0.9058\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - 131s 4s/step - loss: 0.3283 - accuracy: 0.9140\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 130s 4s/step - loss: 0.2779 - accuracy: 0.9266\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 127s 4s/step - loss: 0.2564 - accuracy: 0.9135\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 130s 4s/step - loss: 0.3202 - accuracy: 0.9129\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - 127s 4s/step - loss: 0.2885 - accuracy: 0.9145\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 129s 4s/step - loss: 0.3256 - accuracy: 0.9029\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 129s 4s/step - loss: 0.2877 - accuracy: 0.9301\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 130s 4s/step - loss: 0.2997 - accuracy: 0.9096\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 129s 4s/step - loss: 0.2710 - accuracy: 0.9294\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 131s 4s/step - loss: 0.2738 - accuracy: 0.9248\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 132s 4s/step - loss: 0.2489 - accuracy: 0.9307\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 128s 4s/step - loss: 0.2906 - accuracy: 0.9084\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 131s 4s/step - loss: 0.2730 - accuracy: 0.9281\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 131s 4s/step - loss: 0.2868 - accuracy: 0.9165\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 131s 4s/step - loss: 0.2586 - accuracy: 0.9298\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - 126s 4s/step - loss: 0.3091 - accuracy: 0.9053\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - 130s 4s/step - loss: 0.2496 - accuracy: 0.9313\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - 130s 4s/step - loss: 0.2635 - accuracy: 0.9339\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - 127s 4s/step - loss: 0.3200 - accuracy: 0.9075\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - 130s 4s/step - loss: 0.2961 - accuracy: 0.9156\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - 130s 4s/step - loss: 0.2558 - accuracy: 0.9285\n",
      "Epoch 100/100\n",
      "32/32 [==============================] - 131s 4s/step - loss: 0.2376 - accuracy: 0.9350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2cf00a7a248>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unfreeze all layers for more training\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(train_generator, steps_per_epoch=32, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.reset()\n",
    "print(model.evaluate_generator(test_generator, steps=5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: birds-inceptionv3.model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"birds-inceptionv3.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "\n",
    "ROWS = 256\n",
    "COLS = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = sorted(listdir('train'))\n",
    "model = load_model('birds-inceptionv3.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(fname):\n",
    "    img = image.load_img(fname, target_size=(ROWS, COLS))\n",
    "    img_tensor = image.img_to_array(img) # (height, width, channels)\n",
    "    # (1, height, width, channels), add a dimension because the model expects this shape:\n",
    "    # (batch_size, height, width, channels)\n",
    "    img_tensor = np.expand_dims(img_tensor, axis=0) \n",
    "    img_tensor /= 255. # model expects values in the range [0, 1]\n",
    "    prediction = model.predict(img_tensor)[0]\n",
    "    best_score_index = np.argmax(prediction)\n",
    "    bird = CLASS_NAMES[best_score_index] # retrieve original class name\n",
    "    print(\"Prediction: %s (%.2f%%)\" % (bird, 100*prediction[best_score_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 067.Anna_Hummingbird (86.54%)\n"
     ]
    }
   ],
   "source": [
    "predict('testBirds/annas_hummingbird_sim_1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 198.Rock_Wren (99.63%)\n"
     ]
    }
   ],
   "source": [
    "predict('testBirds/house_wren.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 086.Pacific_Loon (49.33%)\n"
     ]
    }
   ],
   "source": [
    "predict('testBirds/canada_goose_1.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
