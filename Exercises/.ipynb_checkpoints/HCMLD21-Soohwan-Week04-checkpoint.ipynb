{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Youtube Comment Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/Soohwan-Lee/hcmld21/main/Datasets/Youtube01-Psy.csv\"\n",
    "d = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d.query('CLASS == 1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d.query('CLASS == 0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvec = vectorizer.fit_transform(d['CONTENT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<350x1418 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4354 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '02',\n",
       " '034',\n",
       " '05',\n",
       " '08',\n",
       " '10',\n",
       " '100',\n",
       " '100000415527985',\n",
       " '10200253113705769',\n",
       " '1030',\n",
       " '1073741828',\n",
       " '11',\n",
       " '1111',\n",
       " '112720997191206369631',\n",
       " '12',\n",
       " '123',\n",
       " '124',\n",
       " '124923004',\n",
       " '126',\n",
       " '127',\n",
       " '13017194',\n",
       " '131338190916',\n",
       " '1340488',\n",
       " '1340489',\n",
       " '1340490',\n",
       " '1340491',\n",
       " '1340492',\n",
       " '1340493',\n",
       " '1340494',\n",
       " '1340499',\n",
       " '1340500',\n",
       " '1340502',\n",
       " '1340503',\n",
       " '1340504',\n",
       " '1340517',\n",
       " '1340518',\n",
       " '1340519',\n",
       " '1340520',\n",
       " '1340521',\n",
       " '1340522',\n",
       " '1340523',\n",
       " '1340524',\n",
       " '134470083389909',\n",
       " '1415297812',\n",
       " '1495323920744243',\n",
       " '1496241863981208',\n",
       " '1496273723978022',\n",
       " '1498561870415874',\n",
       " '161620527267482',\n",
       " '171183229277',\n",
       " '19',\n",
       " '19924',\n",
       " '1firo',\n",
       " '1m',\n",
       " '20',\n",
       " '2009',\n",
       " '2012',\n",
       " '2012bitches',\n",
       " '2013',\n",
       " '2014',\n",
       " '201470069872822',\n",
       " '2015',\n",
       " '2017',\n",
       " '210',\n",
       " '23',\n",
       " '24',\n",
       " '24398',\n",
       " '243a',\n",
       " '279',\n",
       " '29',\n",
       " '2b',\n",
       " '2billion',\n",
       " '2x10',\n",
       " '300',\n",
       " '3000',\n",
       " '313327',\n",
       " '315',\n",
       " '322',\n",
       " '33',\n",
       " '33gxrf',\n",
       " '39',\n",
       " '390875584405933',\n",
       " '391725794320912',\n",
       " '40beuutvu2zkxk4utgpz8k',\n",
       " '4436607',\n",
       " '4604617',\n",
       " '48051',\n",
       " '484',\n",
       " '492',\n",
       " '4shared',\n",
       " '4snjqp',\n",
       " '4th',\n",
       " '50',\n",
       " '521',\n",
       " '5242575',\n",
       " '5277478',\n",
       " '5287',\n",
       " '57',\n",
       " '58',\n",
       " '5800',\n",
       " '5af506e1',\n",
       " '5c2f',\n",
       " '5million',\n",
       " '5s',\n",
       " '616375350',\n",
       " '636',\n",
       " '6381501',\n",
       " '694',\n",
       " '700',\n",
       " '733634264',\n",
       " '733949243353321',\n",
       " '734237113324534',\n",
       " '74',\n",
       " '750',\n",
       " '783',\n",
       " '79',\n",
       " '821',\n",
       " '884',\n",
       " '898',\n",
       " '8bit',\n",
       " '9082175',\n",
       " '9107',\n",
       " '9277547',\n",
       " '950',\n",
       " '969',\n",
       " '9bzkp7q19f0',\n",
       " '_0f9fa8aa',\n",
       " '__killuminati94',\n",
       " '_bzszz',\n",
       " '_chris_cz',\n",
       " '_fphgk5zllsvdqv0zuf0mb',\n",
       " '_gibu',\n",
       " '_o3h',\n",
       " '_ry6f57sprnd2xv',\n",
       " '_thqbeum69aqup1ih',\n",
       " '_trksid',\n",
       " '_vlczzrg8vgctlpsd9ongewhj8',\n",
       " 'aaaaaaa',\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'absolutely',\n",
       " 'access',\n",
       " 'accessories',\n",
       " 'account',\n",
       " 'accounts',\n",
       " 'acn2g',\n",
       " 'acting',\n",
       " 'active',\n",
       " 'actor',\n",
       " 'actually',\n",
       " 'add',\n",
       " 'adding',\n",
       " 'admit',\n",
       " 'adsense',\n",
       " 'advice',\n",
       " 'affiliateid',\n",
       " 'after',\n",
       " 'again',\n",
       " 'ago',\n",
       " 'ahhh',\n",
       " 'al',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allot',\n",
       " 'allow',\n",
       " 'allways',\n",
       " 'almond',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'alot',\n",
       " 'also',\n",
       " 'am',\n",
       " 'amazing',\n",
       " 'amazon',\n",
       " 'america',\n",
       " 'amount',\n",
       " 'amp',\n",
       " 'an',\n",
       " 'ana',\n",
       " 'and',\n",
       " 'andrijamatf',\n",
       " 'android',\n",
       " 'angel',\n",
       " 'angry',\n",
       " 'animations',\n",
       " 'annoying',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anyway',\n",
       " 'apocalypse',\n",
       " 'app',\n",
       " 'apparel',\n",
       " 'apparently',\n",
       " 'appreciate',\n",
       " 'apps',\n",
       " 'are',\n",
       " 'around',\n",
       " 'art',\n",
       " 'as',\n",
       " 'aseris',\n",
       " 'asia',\n",
       " 'asian',\n",
       " 'aspx',\n",
       " 'ass',\n",
       " 'at',\n",
       " 'attention',\n",
       " 'auburn',\n",
       " 'audiojungle',\n",
       " 'auditioning',\n",
       " 'avaaz',\n",
       " 'avoid',\n",
       " 'aw',\n",
       " 'away',\n",
       " 'aways',\n",
       " 'awesome',\n",
       " 'awesomeness',\n",
       " 'b00ecvf93g',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'bad',\n",
       " 'band',\n",
       " 'bass',\n",
       " 'bd3721315',\n",
       " 'be',\n",
       " 'beautiful',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'behind',\n",
       " 'behold',\n",
       " 'beibs',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'best',\n",
       " 'between',\n",
       " 'bf4',\n",
       " 'bieber',\n",
       " 'big',\n",
       " 'bighit',\n",
       " 'bilion',\n",
       " 'billion',\n",
       " 'billions',\n",
       " 'billon',\n",
       " 'binbox',\n",
       " 'bing',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'blanc',\n",
       " 'block',\n",
       " 'blue',\n",
       " 'bomb',\n",
       " 'book',\n",
       " 'bother',\n",
       " 'bots',\n",
       " 'bottom',\n",
       " 'bowl',\n",
       " 'boyfriend',\n",
       " 'boys',\n",
       " 'bps',\n",
       " 'br',\n",
       " 'brain',\n",
       " 'brand',\n",
       " 'brew',\n",
       " 'bringing',\n",
       " 'brother',\n",
       " 'brotherhood',\n",
       " 'brothers',\n",
       " 'brt0u5',\n",
       " 'bs',\n",
       " 'bubblews',\n",
       " 'bucket',\n",
       " 'burned',\n",
       " 'but',\n",
       " 'butalabs',\n",
       " 'buy',\n",
       " 'by',\n",
       " 'c349',\n",
       " 'call',\n",
       " 'called',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'can',\n",
       " 'cant',\n",
       " 'capitalized',\n",
       " 'car',\n",
       " 'card',\n",
       " 'cards',\n",
       " 'care',\n",
       " 'caroline',\n",
       " 'cash',\n",
       " 'cats',\n",
       " 'cd92db3f4',\n",
       " 'censor',\n",
       " 'certain',\n",
       " 'chacking',\n",
       " 'chainise',\n",
       " 'challenges',\n",
       " 'chance',\n",
       " 'chanel',\n",
       " 'change',\n",
       " 'chanicka',\n",
       " 'channel',\n",
       " 'channels',\n",
       " 'check',\n",
       " 'checked',\n",
       " 'checking',\n",
       " 'chhanel',\n",
       " 'chick',\n",
       " 'child',\n",
       " 'china',\n",
       " 'chinese',\n",
       " 'ching',\n",
       " 'chiptunes',\n",
       " 'christmas',\n",
       " 'chubbz',\n",
       " 'chuck',\n",
       " 'cirus',\n",
       " 'citizen',\n",
       " 'cjfoftxeba',\n",
       " 'cleaning',\n",
       " 'click',\n",
       " 'clicked',\n",
       " 'clip',\n",
       " 'close',\n",
       " 'clothes',\n",
       " 'clothing',\n",
       " 'co',\n",
       " 'code',\n",
       " 'codes',\n",
       " 'codytolleson',\n",
       " 'college',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comentars',\n",
       " 'comes',\n",
       " 'coming',\n",
       " 'comment',\n",
       " 'comment_id',\n",
       " 'commenting',\n",
       " 'comments',\n",
       " 'commercial',\n",
       " 'company',\n",
       " 'complaining',\n",
       " 'completely',\n",
       " 'concerts',\n",
       " 'condition',\n",
       " 'confidence',\n",
       " 'confirmed',\n",
       " 'connected',\n",
       " 'constructive',\n",
       " 'content',\n",
       " 'cool',\n",
       " 'could',\n",
       " 'count',\n",
       " 'countries',\n",
       " 'cover',\n",
       " 'covers',\n",
       " 'craft',\n",
       " 'crap',\n",
       " 'crazy',\n",
       " 'crdits',\n",
       " 'crea',\n",
       " 'credit',\n",
       " 'crew',\n",
       " 'criticism',\n",
       " 'crop',\n",
       " 'cross',\n",
       " 'crowd',\n",
       " 'cs',\n",
       " 'csgo',\n",
       " 'curled',\n",
       " 'cute',\n",
       " 'cvhmklt',\n",
       " 'cxpzpgb',\n",
       " 'czfcxsn0jnq',\n",
       " 'da',\n",
       " 'daaaaaaaaaaannng',\n",
       " 'dad',\n",
       " 'dafuq',\n",
       " 'daily',\n",
       " 'dance',\n",
       " 'dances',\n",
       " 'dancing',\n",
       " 'day',\n",
       " 'dealing',\n",
       " 'dear',\n",
       " 'december',\n",
       " 'decent',\n",
       " 'dedicated',\n",
       " 'defuse',\n",
       " 'deserve',\n",
       " 'designs',\n",
       " 'details',\n",
       " 'dick',\n",
       " 'did',\n",
       " 'diddle',\n",
       " 'didn',\n",
       " 'die',\n",
       " 'difference',\n",
       " 'difficult',\n",
       " 'dinero',\n",
       " 'ding',\n",
       " 'direction',\n",
       " 'disappointed',\n",
       " 'discover',\n",
       " 'dislike',\n",
       " 'dislikes',\n",
       " 'dislikesssssssssssssssssssssssssssssssss',\n",
       " 'divertenti',\n",
       " 'diys',\n",
       " 'dizzy',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'doesnt',\n",
       " 'doing',\n",
       " 'dolacz',\n",
       " 'dominate',\n",
       " 'don',\n",
       " 'donate',\n",
       " 'done',\n",
       " 'dont',\n",
       " 'download',\n",
       " 'downloading',\n",
       " 'dresses',\n",
       " 'dressprettyonce',\n",
       " 'driving',\n",
       " 'drone',\n",
       " 'drones',\n",
       " 'drop',\n",
       " 'drugs',\n",
       " 'drunk',\n",
       " 'dubstep',\n",
       " 'dumb',\n",
       " 'dunno',\n",
       " 'during',\n",
       " 'duty',\n",
       " 'dylan',\n",
       " 'earn',\n",
       " 'earth',\n",
       " 'easily',\n",
       " 'easy',\n",
       " 'easypromosapp',\n",
       " 'ebay',\n",
       " 'ede05ea397ca',\n",
       " 'editor',\n",
       " 'edm',\n",
       " 'eeccon',\n",
       " 'effects',\n",
       " 'effort',\n",
       " 'ehi',\n",
       " 'ejw9kvkoxdamqm808h5z',\n",
       " 'elevator',\n",
       " 'eliminate',\n",
       " 'emerson_zanol',\n",
       " 'end',\n",
       " 'english',\n",
       " 'enimen',\n",
       " 'enjoy',\n",
       " 'enough',\n",
       " 'enter',\n",
       " 'entertaining',\n",
       " 'entire',\n",
       " 'epic',\n",
       " 'equals',\n",
       " 'ermail',\n",
       " 'esteem',\n",
       " 'etc',\n",
       " 'eugenekalinin',\n",
       " 'eve',\n",
       " 'even',\n",
       " 'event',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyday',\n",
       " 'everyone',\n",
       " 'everyones',\n",
       " 'everything',\n",
       " 'ex',\n",
       " 'excuse',\n",
       " 'expectations',\n",
       " 'expensive',\n",
       " 'experiments',\n",
       " 'explore',\n",
       " 'exposure',\n",
       " 'eyebrows',\n",
       " 'eyes',\n",
       " 'f7',\n",
       " 'fablife',\n",
       " 'face',\n",
       " 'facebook',\n",
       " 'facts',\n",
       " 'fail',\n",
       " 'fake',\n",
       " 'family',\n",
       " 'fanboys',\n",
       " 'fantastic',\n",
       " 'far',\n",
       " 'fashionable',\n",
       " 'fb',\n",
       " 'featured',\n",
       " 'feedback',\n",
       " 'feel',\n",
       " 'festival',\n",
       " 'few',\n",
       " 'fighting',\n",
       " 'figure',\n",
       " 'find',\n",
       " 'fire',\n",
       " 'fireball',\n",
       " 'first',\n",
       " 'fish',\n",
       " 'flipagram',\n",
       " 'flute',\n",
       " 'fly',\n",
       " 'follow',\n",
       " 'follower',\n",
       " 'football',\n",
       " 'for',\n",
       " 'forget',\n",
       " 'foto',\n",
       " 'found',\n",
       " 'four',\n",
       " 'freddy',\n",
       " 'free',\n",
       " 'freemyapps',\n",
       " 'french',\n",
       " 'friend',\n",
       " 'friends',\n",
       " 'frigea',\n",
       " 'from',\n",
       " 'fruity',\n",
       " 'fuck',\n",
       " 'fucked',\n",
       " 'fucken',\n",
       " 'fucking',\n",
       " 'fudairyqueen',\n",
       " 'fuego',\n",
       " 'fun',\n",
       " 'funnier',\n",
       " 'funny',\n",
       " 'funnytortspics',\n",
       " 'future',\n",
       " 'gabriel',\n",
       " 'game',\n",
       " 'games',\n",
       " 'gamestop',\n",
       " 'gaming',\n",
       " 'ganga',\n",
       " 'gangam',\n",
       " 'gangman',\n",
       " 'gangnam',\n",
       " 'gangnamstyle',\n",
       " 'gatti',\n",
       " 'gay',\n",
       " 'gbphotographygb',\n",
       " 'gcmforex',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'ghost',\n",
       " 'gift',\n",
       " 'girl',\n",
       " 'girls',\n",
       " 'give',\n",
       " 'giveaways',\n",
       " 'giver',\n",
       " 'glasses',\n",
       " 'go',\n",
       " 'goal',\n",
       " 'goes',\n",
       " 'gofundme',\n",
       " 'going',\n",
       " 'gonna',\n",
       " 'good',\n",
       " 'goodbye',\n",
       " 'google',\n",
       " 'gook',\n",
       " 'got',\n",
       " 'gotta',\n",
       " 'gotten',\n",
       " 'government',\n",
       " 'gp',\n",
       " 'grateful',\n",
       " 'great',\n",
       " 'greetings',\n",
       " 'group',\n",
       " 'grow',\n",
       " 'grwmps',\n",
       " 'gt',\n",
       " 'gta5',\n",
       " 'guardalo',\n",
       " 'gun',\n",
       " 'guy',\n",
       " 'guys',\n",
       " 'gvr7xg',\n",
       " 'gwar',\n",
       " 'hacked',\n",
       " 'hackers',\n",
       " 'hackfbaccountlive',\n",
       " 'had',\n",
       " 'haha',\n",
       " 'hahah',\n",
       " 'hahahahah',\n",
       " 'hair',\n",
       " 'half',\n",
       " 'halftime',\n",
       " 'halp',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'has',\n",
       " 'hassle',\n",
       " 'hate',\n",
       " 'haters',\n",
       " 'hav',\n",
       " 'have',\n",
       " 'having',\n",
       " 'he',\n",
       " 'head',\n",
       " 'headbutt',\n",
       " 'hear',\n",
       " 'heart',\n",
       " 'heck',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hey',\n",
       " 'hi',\n",
       " 'high',\n",
       " 'him',\n",
       " 'hip',\n",
       " 'his',\n",
       " 'history',\n",
       " 'hit',\n",
       " 'hits',\n",
       " 'hl',\n",
       " 'hole',\n",
       " 'holy',\n",
       " 'hop',\n",
       " 'hope',\n",
       " 'hoppa',\n",
       " 'hour',\n",
       " 'how',\n",
       " 'html',\n",
       " 'http',\n",
       " 'https',\n",
       " 'huge',\n",
       " 'huh',\n",
       " 'humanity',\n",
       " 'hunger',\n",
       " 'hw',\n",
       " 'hwang',\n",
       " 'hyperurl',\n",
       " 'hyuna',\n",
       " 'ice',\n",
       " 'id',\n",
       " 'idea',\n",
       " 'ie',\n",
       " 'if',\n",
       " 'ig',\n",
       " 'il',\n",
       " 'ill',\n",
       " 'illuminati',\n",
       " 'im',\n",
       " 'image2you',\n",
       " 'imagine',\n",
       " 'improve',\n",
       " 'in',\n",
       " 'including',\n",
       " 'incmedia',\n",
       " 'indiegogo',\n",
       " 'inspired',\n",
       " 'instagram',\n",
       " 'instagraml',\n",
       " 'internet',\n",
       " 'into',\n",
       " 'inviting',\n",
       " 'io',\n",
       " 'iphone',\n",
       " 'iq2',\n",
       " 'irl',\n",
       " 'is',\n",
       " 'isn',\n",
       " 'isnt',\n",
       " 'it',\n",
       " 'ithat',\n",
       " 'itm',\n",
       " 'its',\n",
       " 'itunes',\n",
       " 'itz',\n",
       " 'jackal',\n",
       " 'jackson',\n",
       " 'jae',\n",
       " 'james',\n",
       " 'jap',\n",
       " 'jaroadc',\n",
       " 'jb',\n",
       " 'jelly',\n",
       " 'jellyfish',\n",
       " 'jenny',\n",
       " 'job',\n",
       " 'join',\n",
       " 'joint2',\n",
       " 'joke',\n",
       " 'joking',\n",
       " 'jr',\n",
       " 'jtcjnmho',\n",
       " 'juice',\n",
       " 'julie',\n",
       " 'julien',\n",
       " 'juno',\n",
       " 'just',\n",
       " 'justin',\n",
       " 'juyk',\n",
       " 'jyp',\n",
       " 'k6a5xt',\n",
       " 'kamal',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'keyword',\n",
       " 'kickstarter',\n",
       " 'kids',\n",
       " 'kidsmediausa',\n",
       " 'kidz',\n",
       " 'kind',\n",
       " 'kinda',\n",
       " 'king',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'knows',\n",
       " 'kobyoshi02',\n",
       " 'kodysman',\n",
       " 'koean',\n",
       " 'kollektivet',\n",
       " 'korea',\n",
       " 'korean',\n",
       " 'koreans',\n",
       " 'kyle',\n",
       " 'l2649',\n",
       " 'l551h',\n",
       " 'la',\n",
       " 'lacked',\n",
       " 'lada',\n",
       " 'language',\n",
       " 'last',\n",
       " 'later',\n",
       " 'laugh',\n",
       " 'launchpad',\n",
       " 'lazy',\n",
       " 'leader',\n",
       " 'league',\n",
       " 'leah',\n",
       " 'learn',\n",
       " 'least',\n",
       " 'leave',\n",
       " 'left',\n",
       " 'let',\n",
       " 'lets',\n",
       " 'lexis',\n",
       " 'like',\n",
       " 'liked',\n",
       " 'likes',\n",
       " 'liking',\n",
       " 'limit',\n",
       " 'ling',\n",
       " 'link',\n",
       " 'linkbucks',\n",
       " 'listen',\n",
       " 'listening',\n",
       " 'listing',\n",
       " 'lists',\n",
       " 'little',\n",
       " 'littlebrother',\n",
       " 'live',\n",
       " 'll',\n",
       " 'lnuj',\n",
       " 'lol',\n",
       " 'long',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'lool',\n",
       " 'loool',\n",
       " 'loops',\n",
       " 'lordviperas',\n",
       " 'lot',\n",
       " 'lots',\n",
       " 'love',\n",
       " 'loved',\n",
       " 'loving',\n",
       " 'low',\n",
       " 'lt',\n",
       " 'lucas',\n",
       " 'lucks',\n",
       " 'luka1qmrhf',\n",
       " 'luther',\n",
       " 'lyrics',\n",
       " 'm1555',\n",
       " 'mabey',\n",
       " 'made',\n",
       " 'make',\n",
       " 'making',\n",
       " 'man',\n",
       " 'management',\n",
       " 'many',\n",
       " 'mario',\n",
       " 'marius',\n",
       " 'market',\n",
       " 'marketglory',\n",
       " 'markusmairhofer',\n",
       " 'martin',\n",
       " 'mathster',\n",
       " 'may',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'mee',\n",
       " 'meets',\n",
       " 'members',\n",
       " 'memories',\n",
       " 'men',\n",
       " 'mercury',\n",
       " 'meselx',\n",
       " 'michael',\n",
       " 'miley',\n",
       " 'milions',\n",
       " 'million',\n",
       " 'millions',\n",
       " 'millioon',\n",
       " 'millisecond',\n",
       " 'millon',\n",
       " 'min',\n",
       " 'mind',\n",
       " 'mine',\n",
       " 'minecraft',\n",
       " 'minoo',\n",
       " 'minutes',\n",
       " 'miss',\n",
       " 'mix',\n",
       " 'model',\n",
       " 'mom',\n",
       " 'mon',\n",
       " 'money',\n",
       " 'monkey',\n",
       " 'monkeys',\n",
       " 'montages',\n",
       " 'month',\n",
       " 'months',\n",
       " 'more',\n",
       " 'morgage',\n",
       " 'moroccan',\n",
       " 'most',\n",
       " 'mother',\n",
       " 'moved',\n",
       " 'movie',\n",
       " 'mp3s',\n",
       " 'ms',\n",
       " 'mscalifornia95',\n",
       " 'msmarilynmiles',\n",
       " 'much',\n",
       " 'multiple',\n",
       " 'murdev',\n",
       " 'music',\n",
       " 'must',\n",
       " 'mute',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'nail',\n",
       " 'nails',\n",
       " 'name',\n",
       " 'national',\n",
       " 'necks',\n",
       " 'need',\n",
       " 'neon',\n",
       " 'net',\n",
       " 'network',\n",
       " 'never',\n",
       " 'new',\n",
       " 'newest',\n",
       " 'news',\n",
       " 'next',\n",
       " 'nice',\n",
       " 'nicushorbboy',\n",
       " 'night',\n",
       " 'nike',\n",
       " 'ninja',\n",
       " 'no',\n",
       " 'non',\n",
       " 'norrus',\n",
       " 'not',\n",
       " 'notch',\n",
       " 'now',\n",
       " 'number',\n",
       " 'numbers',\n",
       " 'obsessed',\n",
       " 'odowd',\n",
       " 'of',\n",
       " 'off',\n",
       " 'offer',\n",
       " 'officialpsy',\n",
       " 'offset',\n",
       " 'offÄ±cal',\n",
       " 'often',\n",
       " 'old',\n",
       " 'older',\n",
       " 'olds',\n",
       " 'oldspice',\n",
       " 'olp_tab_refurbished',\n",
       " 'omg',\n",
       " 'on',\n",
       " 'once',\n",
       " 'oncueapparel',\n",
       " 'one',\n",
       " 'only',\n",
       " 'oppa',\n",
       " 'or',\n",
       " 'org',\n",
       " 'other',\n",
       " 'our',\n",
       " 'out',\n",
       " 'outfit',\n",
       " 'ovbiously',\n",
       " 'over',\n",
       " 'own',\n",
       " 'p3984',\n",
       " 'page',\n",
       " 'pages',\n",
       " 'paid',\n",
       " 'pal',\n",
       " 'pan',\n",
       " 'pants',\n",
       " 'part',\n",
       " 'partners',\n",
       " 'party',\n",
       " 'passed',\n",
       " 'pause',\n",
       " 'pay',\n",
       " 'pazzi',\n",
       " 'pdf',\n",
       " 'pe',\n",
       " 'peace',\n",
       " 'penis',\n",
       " 'people',\n",
       " 'peoples',\n",
       " 'perfect',\n",
       " 'perform',\n",
       " 'permpage',\n",
       " 'person',\n",
       " 'petition',\n",
       " 'petitions',\n",
       " 'phenomena',\n",
       " 'photo',\n",
       " 'photos',\n",
       " 'php',\n",
       " 'pictures',\n",
       " 'piece',\n",
       " 'pivot',\n",
       " 'pl',\n",
       " 'place',\n",
       " 'plan',\n",
       " 'planet',\n",
       " 'platform',\n",
       " 'play',\n",
       " 'please',\n",
       " 'plizz',\n",
       " 'pls',\n",
       " 'plus',\n",
       " 'plz',\n",
       " 'pnref',\n",
       " 'po',\n",
       " 'point',\n",
       " 'pop',\n",
       " 'popaegis',\n",
       " 'popular',\n",
       " 'population',\n",
       " 'populatoin',\n",
       " 'portfolio',\n",
       " 'post',\n",
       " 'posting',\n",
       " 'posts',\n",
       " 'pouring',\n",
       " 'pplease',\n",
       " 'pray',\n",
       " 'praying',\n",
       " 'prehistoric',\n",
       " 'premium',\n",
       " 'pretty',\n",
       " 'preview',\n",
       " 'pride',\n",
       " 'problem',\n",
       " 'prod',\n",
       " 'producer',\n",
       " 'project',\n",
       " 'projects',\n",
       " 'promise',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dshuf = d.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = dshuf[:300]\n",
    "d_test = dshuf[300:]\n",
    "d_train_att = vectorizer.transform(d_train['CONTENT'])\n",
    "d_test_att = vectorizer.transform(d_test['CONTENT'])\n",
    "d_train_label = d_train['CLASS']\n",
    "d_test_label = d_test['CLASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<300x1418 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3650 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_train_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50x1418 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 704 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_test_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=80)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(d_train_att,d_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(d_test_att,d_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26,  1],\n",
       "       [ 2, 21]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pred_labels = clf.predict(d_test_att)\n",
    "confusion_matrix(d_test_label, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(clf, d_train_att, d_train_label, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstUrl = \"https://raw.githubusercontent.com/Soohwan-Lee/hcmld21/main/Datasets/Youtube01-Psy.csv\"\n",
    "secondUrl = \"https://raw.githubusercontent.com/Soohwan-Lee/hcmld21/main/Datasets/Youtube02-KatyPerry.csv\"\n",
    "thridUrl = \"https://raw.githubusercontent.com/Soohwan-Lee/hcmld21/main/Datasets/Youtube03-LMFAO.csv\"\n",
    "fourthUrl = \"https://raw.githubusercontent.com/Soohwan-Lee/hcmld21/main/Datasets/Youtube04-Eminem.csv\"\n",
    "fifthUrl = \"https://raw.githubusercontent.com/Soohwan-Lee/hcmld21/main/Datasets/Youtube05-Shakira.csv\"\n",
    "\n",
    "d = pd.concat([pd.read_csv(firstUrl),\n",
    "              pd.read_csv(secondUrl),\n",
    "              pd.read_csv(thridUrl),\n",
    "              pd.read_csv(fourthUrl),\n",
    "              pd.read_csv(fifthUrl)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1956"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1005"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d.query('CLASS == 1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "951"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d.query('CLASS == 0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dshuf = d.sample(frac=1)\n",
    "d_content = dshuf['CONTENT']\n",
    "d_label = dshuf['CLASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('bag-of-words', CountVectorizer()),\n",
       "                ('random forest', RandomForestClassifier())])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('bag-of-words', CountVectorizer()),\n",
    "    ('random forest', RandomForestClassifier()),\n",
    "])\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('countvectorizer', CountVectorizer()),\n",
       "                ('randomforestclassifier', RandomForestClassifier())])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_pipeline(CountVectorizer(), RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('bag-of-words', CountVectorizer()),\n",
       "                ('random forest', RandomForestClassifier())])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(d_content[:1500],d_label[:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9671052631578947"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(d_content[1500:],d_label[1500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict([\"what a neat video!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict([\"plz subscribe to my channel\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(pipeline, d_content, d_label, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "pipeline2 = make_pipeline(CountVectorizer(), TfidfTransformer(norm=None),RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(pipeline2, d_content, d_label, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() *2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('countvectorizer', CountVectorizer()),\n",
       " ('tfidftransformer', TfidfTransformer(norm=None)),\n",
       " ('randomforestclassifier', RandomForestClassifier())]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline2.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'countvectorizer__max_features': (None, 1000, 2000),\n",
    "    'countvectorizer__ngram_range': ((1, 1), (1, 2)), \n",
    "    'countvectorizer__stop_words': ('english', None),\n",
    "    'tfidftransformer__use_idf': (True, False), \n",
    "    'randomforestclassifier__n_estimators': (20, 50, 100)\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline2, parameters, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:   12.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('countvectorizer', CountVectorizer()),\n",
       "                                       ('tfidftransformer',\n",
       "                                        TfidfTransformer(norm=None)),\n",
       "                                       ('randomforestclassifier',\n",
       "                                        RandomForestClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'countvectorizer__max_features': (None, 1000, 2000),\n",
       "                         'countvectorizer__ngram_range': ((1, 1), (1, 2)),\n",
       "                         'countvectorizer__stop_words': ('english', None),\n",
       "                         'randomforestclassifier__n_estimators': (20, 50, 100),\n",
       "                         'tfidftransformer__use_idf': (True, False)},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(d_content, d_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.961\n",
      "Best parameters set:\n",
      "\tcountvectorizer__max_features: 2000\n",
      "\tcountvectorizer__ngram_range: (1, 1)\n",
      "\tcountvectorizer__stop_words: 'english'\n",
      "\trandomforestclassifier__n_estimators: 20\n",
      "\ttfidftransformer__use_idf: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Test any sentance whether it is spam or not\n",
    "For most comment, it works well, however, it sometimes does not work well for some tricky comments. The last comment was what I wrote to induce people to search my channel. The bag of words model have not data for similarity of words. Also, if there are new type of spam comment style out of existing data set, I think it is hard to figure out whether it is spam comment or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict([\"I think they like me\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict([\"are your dislikes prepared for rewind 2019?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict([\"You should check my channel for Funny VIDEOS!!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict([\"just for test I have to say murdev.com\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict([\"It is time to search 'SWAN STUDIO'!!!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Using trigrams in n-gram-range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'countvectorizer__max_features': (None, 1000, 2000),\n",
    "    'countvectorizer__ngram_range': ((1,3),(1, 3)), # using trigrams\n",
    "    'countvectorizer__stop_words': ('english', None),\n",
    "    'tfidftransformer__use_idf': (True, False), \n",
    "    'randomforestclassifier__n_estimators': (20, 50, 100)\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline2, parameters, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:   12.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('countvectorizer', CountVectorizer()),\n",
       "                                       ('tfidftransformer',\n",
       "                                        TfidfTransformer(norm=None)),\n",
       "                                       ('randomforestclassifier',\n",
       "                                        RandomForestClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'countvectorizer__max_features': (None, 1000, 2000),\n",
       "                         'countvectorizer__ngram_range': ((1, 3), (1, 3)),\n",
       "                         'countvectorizer__stop_words': ('english', None),\n",
       "                         'randomforestclassifier__n_estimators': (20, 50, 100),\n",
       "                         'tfidftransformer__use_idf': (True, False)},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(d_content, d_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.963\n",
      "Best parameters set:\n",
      "\tcountvectorizer__max_features: 2000\n",
      "\tcountvectorizer__ngram_range: (1, 3)\n",
      "\tcountvectorizer__stop_words: 'english'\n",
      "\trandomforestclassifier__n_estimators: 50\n",
      "\ttfidftransformer__use_idf: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Apply the parameters to build a new model\n",
    "I used the unigram or bigrams as shown in the original code, not trigrams on exercise 4.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(best_model, d_content, d_label, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() *2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 Complete the Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim,logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-31 10:31:12,825 : INFO : loading projection weights from https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
      "2021-03-31 10:34:59,742 : INFO : loaded (3000000, 300) matrix from https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n"
     ]
    }
   ],
   "source": [
    "gmodel = gensim.models.KeyedVectors.load_word2vec_format('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0123291 ,  0.20410156, -0.28515625,  0.21679688,  0.11816406,\n",
       "        0.08300781,  0.04980469, -0.00952148,  0.22070312, -0.12597656,\n",
       "        0.08056641, -0.5859375 , -0.00445557, -0.296875  , -0.01312256,\n",
       "       -0.08349609,  0.05053711,  0.15136719, -0.44921875, -0.0135498 ,\n",
       "        0.21484375, -0.14746094,  0.22460938, -0.125     , -0.09716797,\n",
       "        0.24902344, -0.2890625 ,  0.36523438,  0.41210938, -0.0859375 ,\n",
       "       -0.07861328, -0.19726562, -0.09082031, -0.14160156, -0.10253906,\n",
       "        0.13085938, -0.00346375,  0.07226562,  0.04418945,  0.34570312,\n",
       "        0.07470703, -0.11230469,  0.06738281,  0.11230469,  0.01977539,\n",
       "       -0.12353516,  0.20996094, -0.07226562, -0.02783203,  0.05541992,\n",
       "       -0.33398438,  0.08544922,  0.34375   ,  0.13964844,  0.04931641,\n",
       "       -0.13476562,  0.16308594, -0.37304688,  0.39648438,  0.10693359,\n",
       "        0.22167969,  0.21289062, -0.08984375,  0.20703125,  0.08935547,\n",
       "       -0.08251953,  0.05957031,  0.10205078, -0.19238281, -0.09082031,\n",
       "        0.4921875 ,  0.03955078, -0.07080078, -0.0019989 , -0.23046875,\n",
       "        0.25585938,  0.08984375, -0.10644531,  0.00105286, -0.05883789,\n",
       "        0.05102539, -0.0291748 ,  0.19335938, -0.14160156, -0.33398438,\n",
       "        0.08154297, -0.27539062,  0.10058594, -0.10449219, -0.12353516,\n",
       "       -0.140625  ,  0.03491211, -0.11767578, -0.1796875 , -0.21484375,\n",
       "       -0.23828125,  0.08447266, -0.07519531, -0.25976562, -0.21289062,\n",
       "       -0.22363281, -0.09716797,  0.11572266,  0.15429688,  0.07373047,\n",
       "       -0.27539062,  0.14257812, -0.0201416 ,  0.10009766, -0.19042969,\n",
       "       -0.09375   ,  0.14160156,  0.17089844,  0.3125    , -0.16699219,\n",
       "       -0.08691406, -0.05004883, -0.24902344, -0.20800781, -0.09423828,\n",
       "       -0.12255859, -0.09472656, -0.390625  , -0.06640625, -0.31640625,\n",
       "        0.10986328, -0.00156403,  0.04345703,  0.15625   , -0.18945312,\n",
       "       -0.03491211,  0.03393555, -0.14453125,  0.01611328, -0.14160156,\n",
       "       -0.02392578,  0.01501465,  0.07568359,  0.10742188,  0.12695312,\n",
       "        0.10693359, -0.01184082, -0.24023438,  0.0291748 ,  0.16210938,\n",
       "        0.19921875, -0.28125   ,  0.16699219, -0.11621094, -0.25585938,\n",
       "        0.38671875, -0.06640625, -0.4609375 , -0.06176758, -0.14453125,\n",
       "       -0.11621094,  0.05688477,  0.03588867, -0.10693359,  0.18847656,\n",
       "       -0.16699219, -0.01794434,  0.10986328, -0.12353516, -0.16308594,\n",
       "       -0.14453125,  0.12890625,  0.11523438,  0.13671875,  0.05688477,\n",
       "       -0.08105469, -0.06152344, -0.06689453,  0.27929688, -0.19628906,\n",
       "        0.07226562,  0.12304688, -0.20996094, -0.22070312,  0.21386719,\n",
       "       -0.1484375 , -0.05932617,  0.05224609,  0.06445312, -0.02636719,\n",
       "        0.13183594,  0.19433594,  0.27148438,  0.18652344,  0.140625  ,\n",
       "        0.06542969, -0.14453125,  0.05029297,  0.08837891,  0.12255859,\n",
       "        0.26757812,  0.0534668 , -0.32226562, -0.20703125,  0.18164062,\n",
       "        0.04418945, -0.22167969, -0.13769531, -0.04174805, -0.00286865,\n",
       "        0.04077148,  0.07275391, -0.08300781,  0.08398438, -0.3359375 ,\n",
       "       -0.40039062,  0.01757812, -0.18652344, -0.0480957 , -0.19140625,\n",
       "        0.10107422,  0.09277344, -0.30664062, -0.19921875, -0.0168457 ,\n",
       "        0.12207031,  0.14648438, -0.12890625, -0.23535156, -0.05371094,\n",
       "       -0.06640625,  0.06884766, -0.03637695,  0.2109375 , -0.06005859,\n",
       "        0.19335938,  0.05151367, -0.05322266,  0.02893066, -0.27539062,\n",
       "        0.08447266,  0.328125  ,  0.01818848,  0.01495361,  0.04711914,\n",
       "        0.37695312, -0.21875   , -0.03393555,  0.01116943,  0.36914062,\n",
       "        0.02160645,  0.03466797,  0.07275391,  0.16015625, -0.16503906,\n",
       "       -0.296875  ,  0.15039062, -0.29101562,  0.13964844,  0.00448608,\n",
       "        0.171875  , -0.21972656,  0.09326172, -0.19042969,  0.01599121,\n",
       "       -0.09228516,  0.15722656, -0.14160156, -0.0534668 ,  0.03613281,\n",
       "        0.23632812, -0.15136719, -0.00689697, -0.27148438, -0.07128906,\n",
       "       -0.16503906,  0.18457031, -0.08398438,  0.18554688,  0.11669922,\n",
       "        0.02758789, -0.04760742,  0.17871094,  0.06542969, -0.03540039,\n",
       "        0.22949219,  0.02697754, -0.09765625,  0.26953125,  0.08349609,\n",
       "       -0.13085938, -0.10107422, -0.00738525,  0.07128906,  0.14941406,\n",
       "       -0.20605469,  0.18066406, -0.15820312,  0.05932617,  0.28710938,\n",
       "       -0.04663086,  0.15136719,  0.4921875 , -0.27539062,  0.05615234],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmodel['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76094574"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmodel.similarity('cat','dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub(r'<[^>]+>', ' ', sent)\n",
    "    sent = re.sub(r'(\\w)\\'(\\w)', '\\1\\2', sent)\n",
    "    sent = re.sub(r'\\W', ' ', sent) \n",
    "    sent = re.sub(r'\\s+', ' ', sent) \n",
    "    sent = sent.strip()\n",
    "    return sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "unsup_sentences = []\n",
    "\n",
    "for dirname in [\"train/pos\", \"train/neg\", \"train/unsup\", \"test/pos\", \"test/neg\"]:\n",
    "    for fname in sorted(os.listdir(\"aclImdb/\" + dirname)):\n",
    "        if fname[-4:] == '.txt':\n",
    "            with open(\"aclImdb/\" + dirname + \"/\" + fname, encoding='UTF-8') as f:\n",
    "                sent = f.read()\n",
    "                words = extract_words(sent)\n",
    "                unsup_sentences.append(TaggedDocument(words, [dirname + \"/\" + fname]))\n",
    "                \n",
    "for dirname in [\"txt_sentoken/pos\", \"txt_sentoken/neg\"]:\n",
    "    for fname in sorted(os.listdir(dirname)):\n",
    "        if fname[-4:] == '.txt':\n",
    "            with open(dirname + \"/\" + fname, encoding='UTF-8') as f:\n",
    "                for i, sent in enumerate(f):\n",
    "                    words = extract_words(sent)\n",
    "                    unsup_sentences.append(TaggedDocument(words, [\"%s/%s-%d\" % (dirname, fname, i)]))\n",
    "                    \n",
    "with open(\"stanfordSentimentTreebank/original_rt_snippets.txt\", encoding='UTF-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        words = extract_words(sent)\n",
    "        unsup_sentences.append(TaggedDocument(words, [\"rt-%d\" % i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175325"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unsup_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', 'such', 'as', 'teachers', 'my', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', 'hig', 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', 'teachers', 'the', 'scramble', 'to', 'survive', 'financially', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', 'pomp', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', 'i', 'immediately', 'recalled', 'at', 'high', 'a', 'classic', 'line', 'inspector', 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', 'student', 'welcome', 'to', 'bromwell', 'high', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bromwell', 'high', 'is', 'far', 'fetched', 'what', 'a', 'pity', 'that', 'it', 'is'], tags=['train/pos/0_9.txt']),\n",
       " TaggedDocument(words=['homelessness', 'or', 'houselessness', 'as', 'george', 'carlin', 'stated', 'has', 'been', 'an', 'issue', 'for', 'years', 'but', 'never', 'a', 'plan', 'to', 'help', 'those', 'on', 'the', 'street', 'that', 'were', 'once', 'considered', 'human', 'who', 'did', 'everything', 'from', 'going', 'to', 'school', 'work', 'or', 'vote', 'for', 'the', 'matter', 'most', 'people', 'think', 'of', 'the', 'homeless', 'as', 'just', 'a', 'lost', 'cause', 'while', 'worrying', 'about', 'things', 'such', 'as', 'racism', 'the', 'war', 'on', 'iraq', 'pressuring', 'kids', 'to', 'succeed', 'technology', 'the', 'elections', 'inflation', 'or', 'worrying', 'if', 'the', 'l', 'be', 'next', 'to', 'end', 'up', 'on', 'the', 'streets', 'but', 'what', 'if', 'you', 'were', 'given', 'a', 'bet', 'to', 'live', 'on', 'the', 'streets', 'for', 'a', 'month', 'without', 'the', 'luxuries', 'you', 'once', 'had', 'from', 'a', 'home', 'the', 'entertainment', 'sets', 'a', 'bathroom', 'pictures', 'on', 'the', 'wall', 'a', 'computer', 'and', 'everything', 'you', 'once', 'treasure', 'to', 'see', 'what', 'i', 'like', 'to', 'be', 'homeless', 'that', 'is', 'goddard', 'bol', 'lesson', 'mel', 'brooks', 'who', 'directs', 'who', 'stars', 'as', 'bolt', 'plays', 'a', 'rich', 'man', 'who', 'has', 'everything', 'in', 'the', 'world', 'until', 'deciding', 'to', 'make', 'a', 'bet', 'with', 'a', 'sissy', 'rival', 'jeffery', 'tambor', 'to', 'see', 'if', 'he', 'can', 'live', 'in', 'the', 'streets', 'for', 'thirty', 'days', 'without', 'the', 'luxuries', 'if', 'bolt', 'succeeds', 'he', 'can', 'do', 'what', 'he', 'wants', 'with', 'a', 'future', 'project', 'of', 'making', 'more', 'buildings', 'the', 'be', 'on', 'where', 'bolt', 'is', 'thrown', 'on', 'the', 'street', 'with', 'a', 'bracelet', 'on', 'his', 'leg', 'to', 'monitor', 'his', 'every', 'move', 'where', 'he', 'ca', 'step', 'off', 'the', 'sidewalk', 'h', 'given', 'the', 'nickname', 'pepto', 'by', 'a', 'vagrant', 'after', 'i', 'written', 'on', 'his', 'forehead', 'where', 'bolt', 'meets', 'other', 'characters', 'including', 'a', 'woman', 'by', 'the', 'name', 'of', 'molly', 'lesley', 'ann', 'warren', 'an', 'ex', 'dancer', 'who', 'got', 'divorce', 'before', 'losing', 'her', 'home', 'and', 'her', 'pals', 'sailor', 'howard', 'morris', 'and', 'fumes', 'teddy', 'wilson', 'who', 'are', 'already', 'used', 'to', 'the', 'streets', 'the', 'e', 'survivors', 'bolt', 'is', 'h', 'not', 'used', 'to', 'reaching', 'mutual', 'agreements', 'like', 'he', 'once', 'did', 'when', 'being', 'rich', 'where', 'i', 'fight', 'or', 'flight', 'kill', 'or', 'be', 'killed', 'while', 'the', 'love', 'connection', 'between', 'molly', 'and', 'bolt', 'was', 'necessary', 'to', 'plot', 'i', 'found', 'life', 'stinks', 'to', 'be', 'one', 'of', 'mel', 'brooks', 'observant', 'films', 'where', 'prior', 'to', 'being', 'a', 'comedy', 'it', 'shows', 'a', 'tender', 'side', 'compared', 'to', 'his', 'slapstick', 'work', 'such', 'as', 'blazing', 'saddles', 'young', 'frankenstein', 'or', 'spaceballs', 'for', 'the', 'matter', 'to', 'show', 'what', 'i', 'like', 'having', 'something', 'valuable', 'before', 'losing', 'it', 'the', 'next', 'day', 'or', 'on', 'the', 'other', 'hand', 'making', 'a', 'stupid', 'bet', 'like', 'all', 'rich', 'people', 'do', 'when', 'they', 'do', 'know', 'what', 'to', 'do', 'with', 'their', 'money', 'maybe', 'they', 'should', 'give', 'it', 'to', 'the', 'homeless', 'instead', 'of', 'using', 'it', 'like', 'monopoly', 'money', 'or', 'maybe', 'this', 'film', 'will', 'inspire', 'you', 'to', 'help', 'others'], tags=['train/pos/10000_8.txt']),\n",
       " TaggedDocument(words=['brilliant', 'over', 'acting', 'by', 'lesley', 'ann', 'warren', 'best', 'dramatic', 'hobo', 'lady', 'i', 'have', 'ever', 'seen', 'and', 'love', 'scenes', 'in', 'clothes', 'warehouse', 'are', 'second', 'to', 'none', 'the', 'corn', 'on', 'face', 'is', 'a', 'classic', 'as', 'good', 'as', 'anything', 'in', 'blazing', 'saddles', 'the', 'take', 'on', 'lawyers', 'is', 'also', 'superb', 'after', 'being', 'accused', 'of', 'being', 'a', 'turncoat', 'selling', 'out', 'his', 'boss', 'and', 'being', 'dishonest', 'the', 'lawyer', 'of', 'pepto', 'bolt', 'shrugs', 'indifferently', 'a', 'lawyer', 'he', 'says', 'three', 'funny', 'words', 'jeffrey', 'tambor', 'a', 'favorite', 'from', 'the', 'later', 'larry', 'sanders', 'show', 'is', 'fantastic', 'here', 'too', 'as', 'a', 'mad', 'millionaire', 'who', 'wants', 'to', 'crush', 'the', 'ghetto', 'his', 'character', 'is', 'more', 'malevolent', 'than', 'usual', 'the', 'hospital', 'scene', 'and', 'the', 'scene', 'where', 'the', 'homeless', 'invade', 'a', 'demolition', 'site', 'are', 'all', 'time', 'classics', 'look', 'for', 'the', 'legs', 'scene', 'and', 'the', 'two', 'big', 'diggers', 'fighting', 'one', 'bleeds', 'this', 'movie', 'gets', 'better', 'each', 'time', 'i', 'see', 'it', 'which', 'is', 'quite', 'often'], tags=['train/pos/10001_10.txt']),\n",
       " TaggedDocument(words=['this', 'is', 'easily', 'the', 'most', 'underrated', 'film', 'inn', 'the', 'brooks', 'cannon', 'sure', 'its', 'flawed', 'it', 'does', 'not', 'give', 'a', 'realistic', 'view', 'of', 'homelessness', 'unlike', 'say', 'how', 'citizen', 'kane', 'gave', 'a', 'realistic', 'view', 'of', 'lounge', 'singers', 'or', 'titanic', 'gave', 'a', 'realistic', 'view', 'of', 'italians', 'you', 'idiots', 'many', 'of', 'the', 'jokes', 'fall', 'flat', 'but', 'still', 'this', 'film', 'is', 'very', 'lovable', 'in', 'a', 'way', 'many', 'comedies', 'are', 'not', 'and', 'to', 'pull', 'that', 'off', 'in', 'a', 'story', 'about', 'some', 'of', 'the', 'most', 'traditionally', 'reviled', 'members', 'of', 'society', 'is', 'truly', 'impressive', 'its', 'not', 'the', 'fisher', 'king', 'but', 'its', 'not', 'crap', 'either', 'my', 'only', 'complaint', 'is', 'that', 'brooks', 'should', 'have', 'cast', 'someone', 'else', 'in', 'the', 'lead', 'i', 'love', 'mel', 'as', 'a', 'director', 'and', 'writer', 'not', 'so', 'much', 'as', 'a', 'lead'], tags=['train/pos/10002_7.txt']),\n",
       " TaggedDocument(words=['this', 'is', 'not', 'the', 'typical', 'mel', 'brooks', 'film', 'it', 'was', 'much', 'less', 'slapstick', 'than', 'most', 'of', 'his', 'movies', 'and', 'actually', 'had', 'a', 'plot', 'that', 'was', 'followable', 'leslie', 'ann', 'warren', 'made', 'the', 'movie', 'she', 'is', 'such', 'a', 'fantastic', 'under', 'rated', 'actress', 'there', 'were', 'some', 'moments', 'that', 'could', 'have', 'been', 'fleshed', 'out', 'a', 'bit', 'more', 'and', 'some', 'scenes', 'that', 'could', 'probably', 'have', 'been', 'cut', 'to', 'make', 'the', 'room', 'to', 'do', 'so', 'but', 'all', 'in', 'all', 'this', 'is', 'worth', 'the', 'price', 'to', 'rent', 'and', 'see', 'it', 'the', 'acting', 'was', 'good', 'overall', 'brooks', 'himself', 'did', 'a', 'good', 'job', 'without', 'his', 'characteristic', 'speaking', 'to', 'directly', 'to', 'the', 'audience', 'again', 'warren', 'was', 'the', 'best', 'actor', 'in', 'the', 'movie', 'but', 'fume', 'and', 'sailor', 'both', 'played', 'their', 'parts', 'well'], tags=['train/pos/10003_8.txt']),\n",
       " TaggedDocument(words=['this', 'is', 'the', 'comedic', 'robin', 'williams', 'nor', 'is', 'it', 'the', 'quirky', 'insane', 'robin', 'williams', 'of', 'recent', 'thriller', 'fame', 'this', 'is', 'a', 'hybrid', 'of', 'the', 'classic', 'drama', 'without', 'over', 'dramatization', 'mixed', 'with', 'robi', 'new', 'love', 'of', 'the', 'thriller', 'but', 'this', 'is', 'a', 'thriller', 'per', 'se', 'this', 'is', 'more', 'a', 'mystery', 'suspense', 'vehicle', 'through', 'which', 'williams', 'attempts', 'to', 'locate', 'a', 'sick', 'boy', 'and', 'his', 'keeper', 'also', 'starring', 'sandra', 'oh', 'and', 'rory', 'culkin', 'this', 'suspense', 'drama', 'plays', 'pretty', 'much', 'like', 'a', 'news', 'report', 'until', 'willia', 'character', 'gets', 'close', 'to', 'achieving', 'his', 'goal', 'i', 'must', 'say', 'that', 'i', 'was', 'highly', 'entertained', 'though', 'this', 'movie', 'fails', 'to', 'teach', 'guide', 'inspect', 'or', 'amuse', 'it', 'felt', 'more', 'like', 'i', 'was', 'watching', 'a', 'guy', 'williams', 'as', 'he', 'was', 'actually', 'performing', 'the', 'actions', 'from', 'a', 'third', 'person', 'perspective', 'in', 'other', 'words', 'it', 'felt', 'real', 'and', 'i', 'was', 'able', 'to', 'subscribe', 'to', 'the', 'premise', 'of', 'the', 'story', 'all', 'in', 'all', 'i', 'worth', 'a', 'watch', 'though', 'i', 'definitely', 'not', 'friday', 'saturday', 'night', 'fare', 'it', 'rates', 'a', '7', '7', '10', 'from', 'the', 'fiend'], tags=['train/pos/10004_8.txt']),\n",
       " TaggedDocument(words=['yes', 'its', 'an', 'art', 'to', 'successfully', 'make', 'a', 'slow', 'paced', 'thriller', 'the', 'story', 'unfolds', 'in', 'nice', 'volumes', 'while', 'you', 'do', 'even', 'notice', 'it', 'happening', 'fine', 'performance', 'by', 'robin', 'williams', 'the', 'sexuality', 'angles', 'in', 'the', 'film', 'can', 'seem', 'unnecessary', 'and', 'can', 'probably', 'affect', 'how', 'much', 'you', 'enjoy', 'the', 'film', 'however', 'the', 'core', 'plot', 'is', 'very', 'engaging', 'the', 'movie', 'does', 'rush', 'onto', 'you', 'and', 'still', 'grips', 'you', 'enough', 'to', 'keep', 'you', 'wondering', 'the', 'direction', 'is', 'good', 'use', 'of', 'lights', 'to', 'achieve', 'desired', 'affects', 'of', 'suspense', 'and', 'unexpectedness', 'is', 'good', 'very', 'nice', '1', 'time', 'watch', 'if', 'you', 'are', 'looking', 'to', 'lay', 'back', 'and', 'hear', 'a', 'thrilling', 'short', 'story'], tags=['train/pos/10005_7.txt']),\n",
       " TaggedDocument(words=['in', 'this', 'critically', 'acclaimed', 'psychological', 'thriller', 'based', 'on', 'true', 'events', 'gabriel', 'robin', 'williams', 'a', 'celebrated', 'writer', 'and', 'late', 'night', 'talk', 'show', 'host', 'becomes', 'captivated', 'by', 'the', 'harrowing', 'story', 'of', 'a', 'young', 'listener', 'and', 'his', 'adoptive', 'mother', 'toni', 'collette', 'when', 'troubling', 'questions', 'arise', 'about', 'this', 'bo', 'story', 'however', 'gabriel', 'finds', 'himself', 'drawn', 'into', 'a', 'widening', 'mystery', 'that', 'hides', 'a', 'deadly', 'secret', 'according', 'to', 'fil', 'official', 'synopsis', 'you', 'really', 'should', 'stop', 'reading', 'these', 'comments', 'and', 'watch', 'the', 'film', 'now', 'the', 'how', 'did', 'he', 'lose', 'his', 'leg', 'ending', 'with', 'ms', 'collette', 'planning', 'her', 'new', 'life', 'should', 'be', 'chopped', 'off', 'and', 'sent', 'to', 'deleted', 'scenes', 'land', 'i', 'overkill', 'the', 'true', 'nature', 'of', 'her', 'physical', 'and', 'mental', 'ailments', 'should', 'be', 'obvious', 'by', 'the', 'time', 'mr', 'williams', 'returns', 'to', 'new', 'york', 'possibly', 'her', 'blindness', 'could', 'be', 'in', 'question', 'but', 'a', 'revelation', 'could', 'have', 'be', 'made', 'certain', 'in', 'either', 'the', 'highway', 'or', 'video', 'tape', 'scenes', 'the', 'film', 'would', 'benefit', 'from', 'a', 're', 'editing', 'how', 'about', 'a', 'directo', 'cut', 'williams', 'and', 'bobby', 'cannavale', 'as', 'jess', 'do', 'seem', 'initially', 'believable', 'as', 'a', 'couple', 'a', 'scene', 'or', 'two', 'establishing', 'their', 'relationship', 'might', 'have', 'helped', 'set', 'the', 'stage', 'otherwise', 'the', 'cast', 'is', 'exemplary', 'williams', 'offers', 'an', 'exceptionally', 'strong', 'characterization', 'and', 'not', 'a', 'gay', 'impersonation', 'sandra', 'oh', 'as', 'anna', 'joe', 'morton', 'as', 'ashe', 'and', 'rory', 'culkin', 'pete', 'logand', 'are', 'all', 'perfect', 'best', 'of', 'all', 'collett', 'donna', 'belongs', 'in', 'the', 'creepy', 'hall', 'of', 'fame', 'ms', 'oh', 'is', 'correct', 'in', 'saying', 'collette', 'might', 'be', 'you', 'know', 'like', 'that', 'guy', 'from', 'psycho', 'there', 'have', 'been', 'several', 'years', 'when', 'organizations', 'giving', 'acting', 'awards', 'seemed', 'to', 'reach', 'for', 'women', 'due', 'to', 'a', 'slighter', 'dispersion', 'of', 'roles', 'certainly', 'they', 'could', 'have', 'noticed', 'collette', 'with', 'some', 'award', 'consideration', 'she', 'is', 'that', 'good', 'and', 'director', 'patrick', 'stettner', 'definitely', 'evokes', 'hitchcock', 'he', 'even', 'makes', 'getting', 'a', 'sandwich', 'from', 'a', 'vending', 'machine', 'suspenseful', 'finally', 'writers', 'stettner', 'armistead', 'maupin', 'and', 'terry', 'anderson', 'deserve', 'gratitude', 'from', 'flight', 'attendants', 'everywhere', 'the', 'night', 'listener', '1', '21', '06', 'patrick', 'stettner', 'robin', 'williams', 'toni', 'collette', 'sandra', 'oh', 'rory', 'culkin'], tags=['train/pos/10006_7.txt']),\n",
       " TaggedDocument(words=['the', 'night', 'listener', '2006', '1', '2', 'robin', 'williams', 'toni', 'collette', 'bobby', 'cannavale', 'rory', 'culkin', 'joe', 'morton', 'sandra', 'oh', 'john', 'cullum', 'lisa', 'emery', 'becky', 'ann', 'baker', 'dir', 'patrick', 'stettner', 'hitchcockian', 'suspenser', 'gives', 'williams', 'a', 'stand', 'out', 'low', 'key', 'performance', 'what', 'is', 'it', 'about', 'celebrities', 'and', 'fans', 'what', 'is', 'the', 'near', 'paranoia', 'one', 'associates', 'with', 'the', 'other', 'and', 'why', 'is', 'it', 'almost', 'the', 'norm', 'in', 'the', 'latest', 'derange', 'fan', 'scenario', 'based', 'on', 'true', 'events', 'no', 'less', 'williams', 'stars', 'as', 'a', 'talk', 'radio', 'personality', 'named', 'gabriel', 'no', 'one', 'who', 'reads', 'stories', 'h', 'penned', 'over', 'the', 'airwaves', 'and', 'has', 'accumulated', 'an', 'interesting', 'fan', 'in', 'the', 'form', 'of', 'a', 'young', 'boy', 'named', 'pete', 'logand', 'culkin', 'who', 'has', 'submitted', 'a', 'manuscript', 'about', 'the', 'travails', 'of', 'his', 'troubled', 'youth', 'to', 'no', 'on', 'editor', 'ashe', 'morton', 'who', 'gives', 'it', 'to', 'no', 'one', 'to', 'read', 'for', 'himself', 'no', 'one', 'is', 'naturally', 'disturbed', 'but', 'ultimately', 'intrigued', 'about', 'the', 'nightmarish', 'existence', 'of', 'pete', 'being', 'abducted', 'and', 'sexually', 'abused', 'for', 'years', 'until', 'he', 'was', 'finally', 'rescued', 'by', 'a', 'nurse', 'named', 'donna', 'collette', 'giving', 'an', 'excellent', 'performance', 'who', 'has', 'adopted', 'the', 'boy', 'but', 'her', 'correspondence', 'with', 'no', 'one', 'reveals', 'that', 'pete', 'is', 'dying', 'from', 'aids', 'naturally', 'no', 'one', 'wants', 'to', 'meet', 'the', 'fans', 'but', 'is', 'suddenly', 'in', 'doubt', 'to', 'their', 'possibly', 'devious', 'ulterior', 'motives', 'when', 'the', 'seed', 'is', 'planted', 'by', 'his', 'estranged', 'lover', 'jess', 'cannavale', 'whose', 'sudden', 'departure', 'from', 'their', 'new', 'york', 'city', 'apartment', 'has', 'no', 'one', 'in', 'an', 'emotional', 'tailspin', 'that', 'has', 'only', 'now', 'grown', 'into', 'a', 'tempest', 'in', 'a', 'teacup', 'when', 'he', 'decides', 'to', 'do', 'some', 'investigating', 'into', 'donna', 'and', 'pet', 'backgrounds', 'discovering', 'some', 'truths', 'that', 'he', 'did', 'anticipate', 'written', 'by', 'armistead', 'maupin', 'who', 'co', 'wrote', 'the', 'screenplay', 'with', 'his', 'former', 'lover', 'terry', 'anderson', 'and', 'the', 'fil', 'novice', 'director', 'stettner', 'and', 'based', 'on', 'a', 'true', 'story', 'about', 'a', 'fa', 'hoax', 'found', 'out', 'has', 'some', 'hitchcockian', 'moments', 'that', 'run', 'on', 'full', 'tilt', 'like', 'any', 'good', 'old', 'fashioned', 'pot', 'boiler', 'does', 'it', 'helps', 'that', 'williams', 'gives', 'a', 'stand', 'out', 'low', 'key', 'performance', 'as', 'the', 'conflicted', 'good', 'hearted', 'personality', 'who', 'genuinely', 'wants', 'to', 'believe', 'that', 'his', 'number', 'one', 'fan', 'is', 'in', 'fact', 'real', 'and', 'does', 'love', 'him', 'the', 'one', 'thing', 'that', 'has', 'escaped', 'his', 'own', 'reality', 'and', 'has', 'some', 'unsettling', 'dreadful', 'moments', 'with', 'the', 'creepy', 'collette', 'whose', 'one', 'physical', 'trait', 'i', 'will', 'leave', 'unmentioned', 'but', 'underlines', 'the', 'desperation', 'of', 'her', 'character', 'that', 'can', 'rattle', 'you', 'to', 'the', 'core', 'however', 'the', 'film', 'runs', 'out', 'of', 'gas', 'and', 'eventually', 'becomes', 'a', 'bit', 'repetitive', 'and', 'predictable', 'despite', 'a', 'finely', 'directed', 'piece', 'of', 'hoodwink', 'and', 'mystery', 'by', 'stettner', 'it', 'pays', 'to', 'listen', 'to', 'your', 'own', 'inner', 'voice', 'be', 'careful', 'of', 'what', 'you', 'hope', 'for'], tags=['train/pos/10007_7.txt']),\n",
       " TaggedDocument(words=['you', 'know', 'robin', 'williams', 'god', 'bless', 'him', 'is', 'constantly', 'shooting', 'himself', 'in', 'the', 'foot', 'lately', 'with', 'all', 'these', 'dumb', 'comedies', 'he', 'has', 'done', 'this', 'decade', 'with', 'perhaps', 'the', 'exception', 'of', 'death', 'to', 'smoochy', 'which', 'bombed', 'when', 'it', 'came', 'out', 'but', 'is', 'now', 'a', 'cult', 'classic', 'the', 'dramas', 'he', 'has', 'made', 'lately', 'have', 'been', 'fantastic', 'especially', 'insomnia', 'and', 'one', 'hour', 'photo', 'the', 'night', 'listener', 'despite', 'mediocre', 'reviews', 'and', 'a', 'quick', 'dvd', 'release', 'is', 'among', 'his', 'best', 'work', 'period', 'this', 'is', 'a', 'very', 'chilling', 'story', 'even', 'though', 'it', 'does', 'include', 'a', 'serial', 'killer', 'or', 'anyone', 'that', 'physically', 'dangerous', 'for', 'that', 'matter', 'the', 'concept', 'of', 'the', 'film', 'is', 'based', 'on', 'an', 'actual', 'case', 'of', 'fraud', 'that', 'still', 'has', 'yet', 'to', 'be', 'officially', 'confirmed', 'in', 'high', 'school', 'i', 'read', 'an', 'autobiography', 'by', 'a', 'child', 'named', 'anthony', 'godby', 'johnson', 'who', 'suffered', 'horrific', 'abuse', 'and', 'eventually', 'contracted', 'aids', 'as', 'a', 'result', 'i', 'was', 'moved', 'by', 'the', 'story', 'until', 'i', 'read', 'reports', 'online', 'that', 'johnson', 'may', 'not', 'actually', 'exist', 'when', 'i', 'saw', 'this', 'movie', 'the', 'confused', 'feelings', 'that', 'robin', 'williams', 'so', 'brilliantly', 'portrayed', 'resurfaced', 'in', 'my', 'mind', 'toni', 'collette', 'probably', 'gives', 'her', 'best', 'dramatic', 'performance', 'too', 'as', 'the', 'ultimately', 'sociopathic', 'caretaker', 'her', 'role', 'was', 'a', 'far', 'cry', 'from', 'those', 'she', 'had', 'in', 'movies', 'like', 'little', 'miss', 'sunshine', 'there', 'were', 'even', 'times', 'she', 'looked', 'into', 'the', 'camera', 'where', 'i', 'thought', 'she', 'was', 'staring', 'right', 'at', 'me', 'it', 'takes', 'a', 'good', 'actress', 'to', 'play', 'that', 'sort', 'of', 'role', 'and', 'i', 'this', 'understated', 'yet', 'well', 'reviewed', 'role', 'that', 'makes', 'toni', 'collette', 'probably', 'one', 'of', 'the', 'best', 'actresses', 'of', 'this', 'generation', 'not', 'to', 'have', 'even', 'been', 'nominated', 'for', 'an', 'academy', 'award', 'as', 'of', '2008', 'i', 'incredible', 'that', 'there', 'is', 'at', 'least', 'one', 'woman', 'in', 'this', 'world', 'who', 'is', 'like', 'this', 'and', 'i', 'scary', 'too', 'this', 'is', 'a', 'good', 'dark', 'film', 'that', 'i', 'highly', 'recommend', 'be', 'prepared', 'to', 'be', 'unsettled', 'though', 'because', 'this', 'movie', 'leaves', 'you', 'with', 'a', 'strange', 'feeling', 'at', 'the', 'end'], tags=['train/pos/10008_7.txt'])]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsup_sentences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REF: https://github.com/PacktPublishing/Python-Artificial-Intelligence-Projects-for-Beginners/blob/master/Chapter03/SentimentAnalysis.py\n",
    "# I needed to use PermuteSentence Class, thus I googled as following keywords \"python PermuteSentence Class\"\n",
    "import random\n",
    "class PermuteSentences(object):\n",
    "    def __init__(self, sents):\n",
    "        self.sents = sents\n",
    "        \n",
    "    def __iter__(self):\n",
    "        shuffled = list(self.sents)\n",
    "        random.shuffle(shuffled)\n",
    "        for sent in shuffled:\n",
    "            yield sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LeeSooHwan\\anaconda3\\envs\\py37\\lib\\site-packages\\gensim\\models\\doc2vec.py:319: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "2021-03-31 11:19:03,730 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2021-03-31 11:19:03,731 : INFO : collecting all words and their counts\n",
      "2021-03-31 11:19:03,843 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2021-03-31 11:19:04,038 : INFO : PROGRESS: at example #10000, processed 1397256 words (7152515/s), 45097 word types, 10000 tags\n",
      "2021-03-31 11:19:04,226 : INFO : PROGRESS: at example #20000, processed 2812774 words (7588757/s), 61407 word types, 20000 tags\n",
      "2021-03-31 11:19:04,416 : INFO : PROGRESS: at example #30000, processed 4217786 words (7436404/s), 73197 word types, 30000 tags\n",
      "2021-03-31 11:19:04,607 : INFO : PROGRESS: at example #40000, processed 5618059 words (7331487/s), 82645 word types, 40000 tags\n",
      "2021-03-31 11:19:04,794 : INFO : PROGRESS: at example #50000, processed 7027950 words (7543642/s), 90902 word types, 50000 tags\n",
      "2021-03-31 11:19:04,995 : INFO : PROGRESS: at example #60000, processed 8428620 words (6985233/s), 98155 word types, 60000 tags\n",
      "2021-03-31 11:19:05,180 : INFO : PROGRESS: at example #70000, processed 9815353 words (7525181/s), 104630 word types, 70000 tags\n",
      "2021-03-31 11:19:05,380 : INFO : PROGRESS: at example #80000, processed 11220011 words (7040719/s), 110832 word types, 80000 tags\n",
      "2021-03-31 11:19:05,585 : INFO : PROGRESS: at example #90000, processed 12642025 words (6964614/s), 116559 word types, 90000 tags\n",
      "2021-03-31 11:19:05,792 : INFO : PROGRESS: at example #100000, processed 14060005 words (6854310/s), 122185 word types, 100000 tags\n",
      "2021-03-31 11:19:05,992 : INFO : PROGRESS: at example #110000, processed 15450680 words (6968171/s), 127238 word types, 110000 tags\n",
      "2021-03-31 11:19:06,190 : INFO : PROGRESS: at example #120000, processed 16842898 words (7068660/s), 132030 word types, 120000 tags\n",
      "2021-03-31 11:19:06,385 : INFO : PROGRESS: at example #130000, processed 18256177 words (7240077/s), 136549 word types, 130000 tags\n",
      "2021-03-31 11:19:06,587 : INFO : PROGRESS: at example #140000, processed 19689478 words (7096427/s), 140882 word types, 140000 tags\n",
      "2021-03-31 11:19:06,780 : INFO : PROGRESS: at example #150000, processed 21098484 words (7360074/s), 145061 word types, 150000 tags\n",
      "2021-03-31 11:19:06,985 : INFO : PROGRESS: at example #160000, processed 22523672 words (6965898/s), 149240 word types, 160000 tags\n",
      "2021-03-31 11:19:07,204 : INFO : PROGRESS: at example #170000, processed 23941131 words (6493455/s), 153298 word types, 170000 tags\n",
      "2021-03-31 11:19:07,318 : INFO : collected 155374 word types and 175325 unique tags from a corpus of 175325 examples and 24693510 words\n",
      "2021-03-31 11:19:07,318 : INFO : Loading a fresh vocabulary\n",
      "2021-03-31 11:19:07,409 : INFO : effective_min_count=5 retains 57730 unique words (37% of original 155374, drops 97644)\n",
      "2021-03-31 11:19:07,409 : INFO : effective_min_count=5 leaves 24537325 word corpus (99% of original 24693510, drops 156185)\n",
      "2021-03-31 11:19:07,550 : INFO : deleting the raw counts dictionary of 155374 items\n",
      "2021-03-31 11:19:07,554 : INFO : sample=0.001 downsamples 47 most-common words\n",
      "2021-03-31 11:19:07,555 : INFO : downsampling leaves estimated 18578883 word corpus (75.7% of prior 24537325)\n",
      "2021-03-31 11:19:07,602 : INFO : constructing a huffman tree from 57730 words\n",
      "2021-03-31 11:19:08,733 : INFO : built huffman tree with maximum node depth 22\n",
      "2021-03-31 11:19:08,837 : INFO : estimated required memory for 57730 words and 50 dimensions: 145179000 bytes\n",
      "2021-03-31 11:19:08,838 : INFO : resetting layer weights\n",
      "2021-03-31 11:19:40,688 : INFO : training model with 3 workers on 57730 vocabulary and 50 features, using sg=1 hs=1 sample=0.001 negative=5 window=5\n",
      "2021-03-31 11:19:41,705 : INFO : EPOCH 1 - PROGRESS: at 4.92% examples, 909558 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-31 11:19:42,712 : INFO : EPOCH 1 - PROGRESS: at 10.01% examples, 933341 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:19:43,714 : INFO : EPOCH 1 - PROGRESS: at 15.11% examples, 939735 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-31 11:19:44,718 : INFO : EPOCH 1 - PROGRESS: at 20.35% examples, 951453 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:19:45,720 : INFO : EPOCH 1 - PROGRESS: at 25.60% examples, 955051 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:19:46,728 : INFO : EPOCH 1 - PROGRESS: at 31.07% examples, 963719 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-31 11:19:47,742 : INFO : EPOCH 1 - PROGRESS: at 36.33% examples, 965305 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:19:48,746 : INFO : EPOCH 1 - PROGRESS: at 41.35% examples, 963797 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:19:49,752 : INFO : EPOCH 1 - PROGRESS: at 46.20% examples, 957412 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:19:50,762 : INFO : EPOCH 1 - PROGRESS: at 51.41% examples, 957221 words/s, in_qsize 4, out_qsize 1\n",
      "2021-03-31 11:19:51,768 : INFO : EPOCH 1 - PROGRESS: at 56.71% examples, 958703 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:19:52,785 : INFO : EPOCH 1 - PROGRESS: at 61.28% examples, 949682 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:19:53,787 : INFO : EPOCH 1 - PROGRESS: at 66.24% examples, 948088 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:19:54,788 : INFO : EPOCH 1 - PROGRESS: at 71.48% examples, 950729 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:19:55,797 : INFO : EPOCH 1 - PROGRESS: at 76.44% examples, 949366 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:19:56,801 : INFO : EPOCH 1 - PROGRESS: at 81.94% examples, 954974 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:19:57,814 : INFO : EPOCH 1 - PROGRESS: at 87.33% examples, 957414 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:19:58,832 : INFO : EPOCH 1 - PROGRESS: at 92.80% examples, 959429 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:19:59,843 : INFO : EPOCH 1 - PROGRESS: at 98.22% examples, 962293 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:00,152 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-31 11:20:00,153 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-31 11:20:00,155 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-31 11:20:00,156 : INFO : EPOCH - 1 : training on 24693510 raw words (18753314 effective words) took 19.5s, 963675 effective words/s\n",
      "2021-03-31 11:20:01,165 : INFO : EPOCH 2 - PROGRESS: at 4.69% examples, 891079 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-31 11:20:02,172 : INFO : EPOCH 2 - PROGRESS: at 10.10% examples, 951156 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:03,178 : INFO : EPOCH 2 - PROGRESS: at 15.62% examples, 982338 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-31 11:20:04,181 : INFO : EPOCH 2 - PROGRESS: at 21.32% examples, 1004436 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:05,192 : INFO : EPOCH 2 - PROGRESS: at 27.20% examples, 1020750 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:06,193 : INFO : EPOCH 2 - PROGRESS: at 32.97% examples, 1028141 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:07,213 : INFO : EPOCH 2 - PROGRESS: at 38.55% examples, 1028573 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-31 11:20:08,220 : INFO : EPOCH 2 - PROGRESS: at 44.04% examples, 1029248 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:09,221 : INFO : EPOCH 2 - PROGRESS: at 49.92% examples, 1036648 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:10,225 : INFO : EPOCH 2 - PROGRESS: at 55.52% examples, 1039298 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:11,234 : INFO : EPOCH 2 - PROGRESS: at 61.36% examples, 1041744 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:12,239 : INFO : EPOCH 2 - PROGRESS: at 66.94% examples, 1041164 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:13,239 : INFO : EPOCH 2 - PROGRESS: at 72.68% examples, 1042106 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:14,240 : INFO : EPOCH 2 - PROGRESS: at 78.34% examples, 1043832 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:15,241 : INFO : EPOCH 2 - PROGRESS: at 84.05% examples, 1044322 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-31 11:20:16,245 : INFO : EPOCH 2 - PROGRESS: at 89.92% examples, 1046764 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:17,265 : INFO : EPOCH 2 - PROGRESS: at 95.68% examples, 1049010 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:17,992 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-31 11:20:17,994 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-31 11:20:18,007 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-31 11:20:18,008 : INFO : EPOCH - 2 : training on 24693510 raw words (18753288 effective words) took 17.8s, 1050855 effective words/s\n",
      "2021-03-31 11:20:19,019 : INFO : EPOCH 3 - PROGRESS: at 5.08% examples, 937129 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:20,022 : INFO : EPOCH 3 - PROGRESS: at 10.71% examples, 989367 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:21,025 : INFO : EPOCH 3 - PROGRESS: at 16.42% examples, 1016740 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:22,027 : INFO : EPOCH 3 - PROGRESS: at 22.04% examples, 1024198 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:23,032 : INFO : EPOCH 3 - PROGRESS: at 27.70% examples, 1028702 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:24,037 : INFO : EPOCH 3 - PROGRESS: at 33.48% examples, 1037939 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:25,038 : INFO : EPOCH 3 - PROGRESS: at 39.26% examples, 1046350 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:26,040 : INFO : EPOCH 3 - PROGRESS: at 44.94% examples, 1049576 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:27,043 : INFO : EPOCH 3 - PROGRESS: at 50.73% examples, 1052746 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-31 11:20:28,046 : INFO : EPOCH 3 - PROGRESS: at 56.38% examples, 1052520 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:29,048 : INFO : EPOCH 3 - PROGRESS: at 62.17% examples, 1056518 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:30,052 : INFO : EPOCH 3 - PROGRESS: at 67.92% examples, 1058173 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:31,055 : INFO : EPOCH 3 - PROGRESS: at 73.75% examples, 1059919 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:32,064 : INFO : EPOCH 3 - PROGRESS: at 79.42% examples, 1060779 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:33,068 : INFO : EPOCH 3 - PROGRESS: at 85.26% examples, 1061514 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:34,075 : INFO : EPOCH 3 - PROGRESS: at 91.01% examples, 1062099 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-31 11:20:35,083 : INFO : EPOCH 3 - PROGRESS: at 96.91% examples, 1064486 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-31 11:20:35,595 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-31 11:20:35,604 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-31 11:20:35,604 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-31 11:20:35,604 : INFO : EPOCH - 3 : training on 24693510 raw words (18753376 effective words) took 17.6s, 1066101 effective words/s\n",
      "2021-03-31 11:20:36,619 : INFO : EPOCH 4 - PROGRESS: at 5.23% examples, 976907 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-31 11:20:37,630 : INFO : EPOCH 4 - PROGRESS: at 10.96% examples, 1028291 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:38,635 : INFO : EPOCH 4 - PROGRESS: at 16.90% examples, 1052250 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:39,641 : INFO : EPOCH 4 - PROGRESS: at 23.05% examples, 1070883 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-31 11:20:40,650 : INFO : EPOCH 4 - PROGRESS: at 28.99% examples, 1078414 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:41,654 : INFO : EPOCH 4 - PROGRESS: at 35.09% examples, 1086703 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:42,656 : INFO : EPOCH 4 - PROGRESS: at 40.81% examples, 1086136 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:43,657 : INFO : EPOCH 4 - PROGRESS: at 46.72% examples, 1089393 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:44,659 : INFO : EPOCH 4 - PROGRESS: at 52.58% examples, 1089728 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:45,660 : INFO : EPOCH 4 - PROGRESS: at 58.71% examples, 1093168 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-31 11:20:46,663 : INFO : EPOCH 4 - PROGRESS: at 64.67% examples, 1095433 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-31 11:20:47,665 : INFO : EPOCH 4 - PROGRESS: at 70.40% examples, 1094639 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:48,666 : INFO : EPOCH 4 - PROGRESS: at 76.28% examples, 1096069 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:49,667 : INFO : EPOCH 4 - PROGRESS: at 82.20% examples, 1097069 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:50,668 : INFO : EPOCH 4 - PROGRESS: at 88.05% examples, 1097755 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:51,672 : INFO : EPOCH 4 - PROGRESS: at 93.98% examples, 1097963 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:52,681 : INFO : EPOCH 4 - PROGRESS: at 99.78% examples, 1096571 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:52,704 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-31 11:20:52,705 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-31 11:20:52,708 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-31 11:20:52,708 : INFO : EPOCH - 4 : training on 24693510 raw words (18755924 effective words) took 17.1s, 1097030 effective words/s\n",
      "2021-03-31 11:20:53,718 : INFO : EPOCH 5 - PROGRESS: at 5.19% examples, 984279 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:54,718 : INFO : EPOCH 5 - PROGRESS: at 10.98% examples, 1040720 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:55,731 : INFO : EPOCH 5 - PROGRESS: at 17.02% examples, 1065972 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:56,735 : INFO : EPOCH 5 - PROGRESS: at 22.93% examples, 1072731 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:20:57,740 : INFO : EPOCH 5 - PROGRESS: at 28.56% examples, 1069440 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-31 11:20:58,744 : INFO : EPOCH 5 - PROGRESS: at 34.41% examples, 1073089 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-31 11:20:59,748 : INFO : EPOCH 5 - PROGRESS: at 40.25% examples, 1077027 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:21:00,752 : INFO : EPOCH 5 - PROGRESS: at 46.20% examples, 1080201 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:21:01,752 : INFO : EPOCH 5 - PROGRESS: at 52.13% examples, 1083753 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:21:02,765 : INFO : EPOCH 5 - PROGRESS: at 57.83% examples, 1080588 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:21:03,769 : INFO : EPOCH 5 - PROGRESS: at 63.68% examples, 1083299 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:21:04,775 : INFO : EPOCH 5 - PROGRESS: at 69.51% examples, 1083955 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:21:05,777 : INFO : EPOCH 5 - PROGRESS: at 75.58% examples, 1086364 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:21:06,781 : INFO : EPOCH 5 - PROGRESS: at 81.49% examples, 1087782 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:21:07,786 : INFO : EPOCH 5 - PROGRESS: at 87.28% examples, 1087104 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:21:08,798 : INFO : EPOCH 5 - PROGRESS: at 93.28% examples, 1087502 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-31 11:21:09,807 : INFO : EPOCH 5 - PROGRESS: at 99.23% examples, 1088502 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-31 11:21:09,938 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-31 11:21:09,942 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-31 11:21:09,942 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-31 11:21:09,943 : INFO : EPOCH - 5 : training on 24693510 raw words (18753312 effective words) took 17.2s, 1088535 effective words/s\n",
      "2021-03-31 11:21:09,943 : INFO : training on a 123467550 raw words (93769214 effective words) took 89.3s, 1050576 effective words/s\n"
     ]
    }
   ],
   "source": [
    "permuter = PermuteSentences(unsup_sentences) \n",
    "model = Doc2Vec(permuter, dm=0, hs=1, size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_temporary_training_data(keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-31 11:21:10,021 : INFO : saving Doc2Vec object under review.d2v, separately None\n",
      "2021-03-31 11:21:11,590 : INFO : saved review.d2v\n"
     ]
    }
   ],
   "source": [
    "model.save('review.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.2354145 , -0.0747091 ,  0.23725848, -0.16804855,  0.03809171,\n",
       "       -0.26735035,  0.10125965,  0.03975399, -0.0538856 ,  0.15884551,\n",
       "       -0.10138144, -0.346097  ,  0.38475174,  0.06646588, -0.296666  ,\n",
       "        0.02790331,  0.12379564, -0.10781914,  0.129481  , -0.08635166,\n",
       "        0.14061613, -0.10235769, -0.23111059,  0.07314836,  0.22970548,\n",
       "       -0.02419515, -0.12309412,  0.40725428, -0.04017804, -0.08313112,\n",
       "       -0.35634443,  0.01005906, -0.0443926 , -0.06115242,  0.41289884,\n",
       "       -0.01377894,  0.16390868, -0.16181411,  0.14085692,  0.03785241,\n",
       "        0.09464942, -0.36501098,  0.4251935 ,  0.08761124, -0.2813565 ,\n",
       "       -0.08492973,  0.0023178 , -0.18295443, -0.02106301,  0.04326778],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector(extract_words(\"This place is not worth your time, let alone Vegas.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50089896]], dtype=float32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(\n",
    "    [model.infer_vector(extract_words(\"This place is not worth your time, let alone Vegas.\"))],\n",
    "    [model.infer_vector(extract_words(\"Service sucks\"))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26085353]], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(\n",
    "    [model.infer_vector(extract_words(\"Highly recommended\"))],\n",
    "    [model.infer_vector(extract_words(\"Service sucks\"))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "sentvecs = []\n",
    "sentiments = []\n",
    "for fname in [\"yelp\", \"amazon_cells\", \"imdb\"]: \n",
    "    with open(\"sentiment labelled sentences/%s_labelled.txt\" % fname, encoding='UTF-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line_split = line.strip().split('\\t')\n",
    "            sentences.append(line_split[0])\n",
    "            words = extract_words(line_split[0])\n",
    "            sentvecs.append(model.infer_vector(words, steps=10))\n",
    "            sentiments.append(int(line_split[1]))\n",
    "            \n",
    "combined = list(zip(sentences, sentvecs, sentiments))\n",
    "random.shuffle(combined)\n",
    "sentences, sentvecs, sentiments = zip(*combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=9)\n",
    "clfrf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7866666666666666, 0.013291601358251255)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(clf, sentvecs, sentiments, cv=5)\n",
    "np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.782, 0.013182479955523478)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(clfrf, sentvecs, sentiments, cv=5)\n",
    "np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "pipeline = make_pipeline(CountVectorizer(), TfidfTransformer(), RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7756666666666666, 0.009043106644167017)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(pipeline, sentences, sentiments, cv=5)\n",
    "np.mean(scores), np.std(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
